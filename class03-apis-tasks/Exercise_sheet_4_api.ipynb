{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise sheet \\#4\n",
    "## Using APIs\n",
    "### Exercise 1\n",
    "Write a Python script which gets the list of astronauts who are currently in Space. To do so, you can use the [astro.json](http://api.open-notify.org/astros.json) OpenNotify API. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All attributes available for wikipedia pages: \n",
    "\n",
    "self.content  \n",
    "self.coordinates  \n",
    "self.images  \n",
    "self.links  \n",
    "self.references  \n",
    "self.section(section_title)  \n",
    "self.sections  \n",
    "self.summary  \n",
    "self.revision_id  \n",
    "self.html()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sergey Prokopyev', 'Dmitry Petelin', 'Frank Rubio', 'Nicole Mann', 'Josh Cassada', 'Koichi Wakata', 'Anna Kikina', 'Fei Junlong', 'Deng Qingming', 'Zhang Lu', 'Stephen Bowen', 'Warren Hoburg', 'Sultan Alneyadi', 'Andrey Fedyaev']\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://api.open-notify.org/astros.json\"\n",
    "ua = {\"User-agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "page = requests.get(url, ua)\n",
    "d = json.loads(page.content) #dict\n",
    "\n",
    "astronauts = [astr[\"name\"] for astr in d[\"people\"]]\n",
    "print(astronauts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "#### Question 2.1\n",
    "Write a Python program, which, for each astronaut found in Exercise 1, retrieves  their (English) wikipedia article and extract the article's summary and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bleuze3u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\bleuze3u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "astronauts_dict = {}\n",
    "# First: check if we can retrieve the pages (they exist, and there is no ambiguity)\n",
    "for astronaut in astronauts:\n",
    "\n",
    "    try:\n",
    "        article = wikipedia.page(title=astronaut, auto_suggest=False)\n",
    "        astronauts_dict[astronaut] = {\"article\": article, \"links\": article.links, \"summary\": article.summary}\n",
    "\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        \n",
    "        try:\n",
    "            article = wikipedia.page(title=astronaut+\" (cosmonaut)\", auto_suggest=False)\n",
    "            astronauts_dict[astronaut] = {\"article\": article, \"links\": article.links, \"summary\": article.summary}\n",
    "        except  wikipedia.PageError as e:\n",
    "            \n",
    "            try:\n",
    "                article = wikipedia.page(title=astronaut+\" (astronaut)\", auto_suggest=False)\n",
    "                astronauts_dict[astronaut] = {\"article\": article, \"links\": article.links, \"summary\": article.summary}\n",
    "            except wikipedia.PageError as e:\n",
    "                astronauts_dict[astronaut] = {\"article\": None, \"links\": None, \"summary\": None}\n",
    "        \n",
    "    except wikipedia.PageError as pe:\n",
    "        astronauts_dict[astronaut] = {\"article\": None, \"links\": None, \"summary\": None}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sergey Prokopyev <WikipediaPage 'Sergey Prokopyev (cosmonaut)'>\n",
      "Dmitry Petelin <WikipediaPage 'Dmitry Petelin'>\n",
      "Frank Rubio <WikipediaPage 'Francisco Rubio (astronaut)'>\n",
      "Nicole Mann <WikipediaPage 'Nicole Aunapu Mann'>\n",
      "Josh Cassada <WikipediaPage 'Josh A. Cassada'>\n",
      "Koichi Wakata <WikipediaPage 'Koichi Wakata'>\n",
      "Anna Kikina <WikipediaPage 'Anna Kikina'>\n",
      "Fei Junlong <WikipediaPage 'Fei Junlong'>\n",
      "Deng Qingming <WikipediaPage 'Deng Qingming'>\n",
      "Zhang Lu <WikipediaPage 'Zhang Lu (taikonaut)'>\n",
      "Stephen Bowen <WikipediaPage 'Stephen Bowen (astronaut)'>\n",
      "Warren Hoburg <WikipediaPage 'Warren Hoburg'>\n",
      "Sultan Alneyadi None\n",
      "Andrey Fedyaev <WikipediaPage 'Andrey Fedyaev'>\n"
     ]
    }
   ],
   "source": [
    "for a in astronauts:\n",
    "    print(a, astronauts_dict[a][\"article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://upload.wikimedia.org/wikipedia/commons/f/fa/Flag_of_the_People%27s_Republic_of_China.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/c/cb/Flag_of_the_United_Arab_Emirates.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/e/ed/ISS_Expedition_56_Patch.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/c/c8/ISS_Expedition_57_Patch.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/d/d3/ISS_Expedition_67_Patch.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/2/2e/ISS_Expedition_68_Patch.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/a/a1/ISS_Expedition_69_Patch.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/c/c7/Sergey_Prokopyev_-_NASA_portrait.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/4/4a/Commons-logo.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/9/9e/Flag_of_Japan.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/f/f3/Flag_of_Russia.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astronauts_dict[astronauts[0]][\"article\"].images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2\n",
    "Extend your Python program so that it only keeps links that are pointing to wikipedia pages (in any language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3\n",
    "Extend your Python program so that it processes these links as follows:\n",
    " - it retrieves the corresponding article and then extracts its references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.4\n",
    "Extend your Python program to compute the average number of views for each astronaut's main article."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wptools\n",
    "page = wptools.page(astronauts[0], lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18860\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# views for article about Albert Einstein in October 2015\n",
    "test_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Albert_Einstein/daily/2015100100/2015103100\"\n",
    "nb = requests.get(test_url, headers = ua)\n",
    "print(nb.json()[\"items\"][0][\"views\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.5\n",
    "Export the extracted information in a CSV file having the following fields:\n",
    "\n",
    "`Astronaut's name ; Article's summary ; links separated by commas ; number of views`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "#### Question 3.1 \n",
    "Using Wikipedia, compile the list of UEFA's Intertoto cup winners sorted by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2\n",
    "Compute the number of Intertoto winners per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Let's play with New York Times API! Register here to get your API access token: https://developer.nytimes.com/apis\n",
    "\n",
    "#### Question 4.1\n",
    "\n",
    "Extract for each astronaut articles in NYT mentioning them from the year 2022. Create a dictionay where by the astronauts name you could get information on number of articles published about them in 2022, links to the last 10 articles of 2022, and list of keywords which are associated with those articles, including how many times each keyword appeared. \n",
    "\n",
    "For example \n",
    "\n",
    "    {\"John Smith\": {\"articles\": [\"nytimes.com/The_Best_Astronaut\",\n",
    "                                 \"nytimes.com/Do_we_really_believe_in_space_travel\"],\n",
    "                   \"num_articles\": 2,\n",
    "                   \"keywords\": {\"space\": 2, \"celebrities\": 1}}}\n",
    "                   \n",
    "<span style=\"color:red\">ATTENTION!</span> Never publish your API access key on github or any other open website! It is only to be used by you. (Well, otherwise the API service could ban you :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.2. Bonus :) \n",
    "\n",
    "What happens when we want to extract links to all the articles and not just the last 10? Extend your program to iterate through pages of the query results :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
