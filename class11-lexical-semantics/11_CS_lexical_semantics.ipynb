{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentence segmentation on the files that occur in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('../07_linguistic_preprocessing/webnlg-test.txt') as infile:\n",
    "            text = infile.read()\n",
    "            \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = sent_tokenize(text)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector  with 10 dimensions , all of value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vector = np.zeros(10)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vocab = [\"a\",\"b\"]\n",
    "print(vocab)\n",
    "vocab_len = len(vocab)\n",
    "df = pd.DataFrame(data=np.zeros((vocab_len, vocab_len)), dtype=np.int16,index=vocab,columns=vocab)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary using defaultdict\n",
    "\n",
    "This avoids having python throw a KeyError when you try to get an item with a key that is not currently in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(int)\n",
    "print(d.items())\n",
    "\n",
    "d[\"the\"] += 1\n",
    "print(d.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary mapping each token in a corpus to a distinct integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "texts = [\"john runs.\", \"Mary swims\"]\n",
    "\n",
    "# Create an empty dictionary using collections.defaultdic method\n",
    "# Set the default value that will be assigned to each token to the current size of the vocabulary\n",
    "token2int = collections.defaultdict(lambda: len(token2int)) \n",
    "\n",
    "# Set the value of <eos> to 0\n",
    "token2int['<eos>'] = 0\n",
    "\n",
    "# Add each new word in \"texts\" to the dictionary\n",
    "# and map it to the integer corresponding to its first position in the text (= the size of\n",
    "# the vocabulary present in the dictionary at that time)\n",
    "for text in texts:\n",
    "    [token2int[token] for token in text.split()]\n",
    "    \n",
    "token2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating, visualising and inspecting a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Shakespeare wrote plays\",\"Shakespeare wrote poems\",\n",
    "        \"Hugo wrote novels\",\"Verne wrote novels\"\n",
    "        \"Rimbaud wrote poems\",\n",
    "        \"John read science\", \n",
    "        \"Peter read books\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a frequency vectorizer object\n",
    "# using  the option \"stop_words = 'english'\" ensures that  stop_words are removed\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words = 'english') \n",
    "\n",
    "## Create a document-term matrix from a list of strings\n",
    "doc_term_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Print out the document / token matrix\n",
    "# use the todense() attribute to create the matrix view\n",
    "print(doc_term_matrix.todense()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show doc-term matrix (rows are documents, columns are terms)\n",
    "print(doc_term_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary extracted by fit_transform\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get token-to-int dictionary\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of a token\n",
    "vectorizer.vocabulary_['hugo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create document-token matrix for a larger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../07_linguistic_preprocessing/webnlg-test.txt') as infile:\n",
    "            text = infile.read()\n",
    "            \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = sent_tokenize(text)\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a frequency vectorizer object\n",
    "# using  the option \"stop_words = 'english'\" ensures that stop_words are removed\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words = 'english') \n",
    "\n",
    "# Convert documents to document/token matrix by applying the vectoriser\n",
    "doc_token_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print out the document / token matrix\n",
    "# use the todense() attribute to create the matrix view\n",
    "print(doc_token_matrix.todense()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get number of rows and columns\n",
    "# because it is a document/token matrix, the shape is (nb of docs, nb of tokens)\n",
    "doc_token_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_['mata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SVD decomposition to a matrix\n",
    "\n",
    "[svd method](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the token co-occurence matrix \n",
    "token_cooc_matrix = (doc_token_matrix.T * doc_token_matrix) \n",
    "# Set the diagonal to 0  (else it will indicate the token count)\n",
    "token_cooc_matrix.setdiag(0) \n",
    "# Print out the token co-occurence matrix\n",
    "print(token_cooc_matrix.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The token cooccurences matrix is square, its shape is (nb of tokens, nb of tokens)\n",
    "token_cooc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# matrix is a word co-occurence matrix\n",
    "# On large corpora, make sure to use the full_matrices=False (reduced SVD) option\n",
    "# else processing will be very slow\n",
    "matrix = token_cooc_matrix.todense()\n",
    "U, S, Vt = np.linalg.svd(matrix,full_matrices=False)\n",
    "# Keep only the first 50 dimensions of U as word vectors\n",
    "U = U[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vector for word 'mata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(U[vectorizer.vocabulary_['mata']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the word cooccurence matrix in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# get the tokens\n",
    "names = vectorizer.get_feature_names() \n",
    "# create a pandas frame whose content is the token co-occurence matrix\n",
    "# and whose row and column headers are the tokens\n",
    "# Note that the matrix input to Pandas must be in dense format\n",
    "df = pd.DataFrame(data = token_cooc_matrix.todense(), columns = names, index = names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out a word vector\n",
    "# Define a dictionary mapping tokens to indice\n",
    "vocab_size = df.shape[0]\n",
    "word2index = dict(zip(df.index,range(vocab_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the vector for \"victory\"\n",
    "print(U[word2index['mata']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary mapping tokens to integer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"a\",\"b\"]\n",
    "\n",
    "dict(zip(tokens,range(len(tokens))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
