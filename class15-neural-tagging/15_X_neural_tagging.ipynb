{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sB9EwaZbbKMN"
   },
   "source": [
    "# Exercise Lecture \"15: Neural Sequence Tagging\"\n",
    "\n",
    "In this assignment, we learn a model which can detect noun phrases referring to visual entities given the **Flicker30k** entities corpus as training data.\n",
    "\n",
    "In this corpus, each word is labelled with either **(B)** if that word starts an NP, **(I)** if it occurs within an NP and **(O)** otherwise. There is one word and one label per line. Sentences are separated by blank lines. \n",
    "\n",
    "> Data file:  f30kE-captions-bio.txt \n",
    "\n",
    "You will also extend the RNN model defined in the previous Exercise by downloading pretrained English **FastText** embeddings, and selecting the set of pretrained vectors associated with the vocabulary of the _f30kE-captions-bio.txt_ file, to use as the embedding layer weights for the RNN model.\n",
    "\n",
    "> Pretrained FastText: wiki-news-300d-1M.vec.zip downloadable [here](https://fasttext.cc/docs/en/english-vectors.html) NOTE: this zip file is 2.26GB after unzipping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecqfYZ4kbKMh"
   },
   "source": [
    "#### Exercise 1 - Creating a list of lists of tokens (one list per sentence) and the corresponding lists of labels\n",
    "\n",
    "* From the input file, create two lists called \"texts\" and \"labels\"\n",
    "    * \"text\" is a list of lists, each list containing the tokens of a sentence\n",
    "    * \"labels\" contains the list of lists of labels for each sentence in \"text\"\n",
    "\n",
    "Note: each line in the file is a token and its label. Sentences are separated by an empty line. Your processing should take this into account to recover the full sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "with open(\"f30kE-captions-bio.txt\", \"r\") as f:\n",
    "    text, labels = [], []\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = f.read().split(\"\\n\\n\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "        text_l, labels_l = [], []\n",
    "        \n",
    "        # Split the sentence into words\n",
    "        for word in sentence.split(\"\\n\"):\n",
    "            split_ = word.split(\" \")\n",
    "            if len(split_) > 1:\n",
    "                token, label = split_[0], split_[1]\n",
    "                text_l.append(token)\n",
    "                labels_l.append(label)\n",
    "            \n",
    "        text_l.append(\"\\n\")\n",
    "        labels_l.append(\"EOS\")\n",
    "        \n",
    "        text.append(text_l)\n",
    "        labels.append(labels_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 5501)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['A',\n",
       "  'group',\n",
       "  'of',\n",
       "  '5',\n",
       "  'scuba',\n",
       "  'divers',\n",
       "  'talk',\n",
       "  'on',\n",
       "  'the',\n",
       "  'surface',\n",
       "  'next',\n",
       "  'to',\n",
       "  'a',\n",
       "  'barrier',\n",
       "  'island',\n",
       "  '.',\n",
       "  '\\n'],\n",
       " ['B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'EOS'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0], labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuckS_TsbKMy"
   },
   "source": [
    "#### Exercise 2 -  Mapping labels to integers and sequence of labels to sequence of integers\n",
    "\n",
    "* Create a dictionary label2int which maps each label to a distinct integer\n",
    "* Apply this dictionary to the list of labels extracted in the previous exercise \n",
    "* Make sure to include an \"\\<eos>\" token in your vocabulary\n",
    "\n",
    "**Hint:** We did this in the preceding lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"O\": 0, \"B\": 1, \"I\": 2, \"EOS\": 3}\n",
    "int_labels = [[label2int[label] for label in labels_l] for labels_l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'EOS'],\n",
       " [1, 2, 2, 2, 2, 2, 0, 0, 1, 2, 0, 0, 1, 2, 2, 0, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0], int_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmX8f6yUbKM5"
   },
   "source": [
    "#### Exercise 3 - Convert the tokens to integers\n",
    "\n",
    "* Similarly define a token2int dictionary mapping each token in your corpus to an integer and use this dictionary to convert the texts in the list \"texts\" (cf. Exercise 1 above) into lists of integers, each integer representing a token\n",
    "\n",
    "**IMPORTANT** make sure to lowercase the tokens as the pre-trained embeddings we'll be using only include lowercased tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4596\n",
      "some words: ['house', 'roaming', 'snowshoes', 'helps', 'dunks', 'specific', 'sparing', 'shrubs', 'outdated', 'crowded', 'day', 'chooses', 'him', 'competitive', 'violins', 'curve', 'boarding', 'matching', 'swims', 'tournament', 'hiker', 'raincoats', 'what', 'loaf', 'got', 'fashioned', 'crocodile', 'tandem', 'card', 'biden', '&', 'drying', 'zara', 'twirling', 'wheelchair', 'courtyard', 'homeless', 'animal', 'well-lit', 'oceanside', 'instructor', 'dancer', 'attendant', 'scooter', 'sandy', 'flame', 'law', 'almost', 'surrounding', 'aa', 'some', 'strips', 'produce', 'puppet', 'geometrical', 'bean', 'walking', 'chair', 'ties', 'macaroni', 'rail', 'playground', 'rear', 'urban', 'actions', 'white-haired', 'snowbank', 'stance', 'paddling', 'cyclist', 'rodeo', 'attending', 'blackboard', 'alone', 'interesting', 'exchange', 'similarly', 'scrubbing', 'floral', 'lead', 'racetrack', 'commuters', 'dread', 'welds', 'shoulders', 'challenged', 'pitch', 'waffle', 'pizza', 'punch', 'brothers', 'puppy', 'marshy', 'lunch', 'vuitton', 'over-sized', 'civilians', 'newborn', 'sweaty', 'made']\n"
     ]
    }
   ],
   "source": [
    "voc_set = set([word.lower() for text_l in text for word in text_l])\n",
    "voc_list = list(voc_set)\n",
    "\n",
    "print(\"Vocabulary size:\", len(voc_set))\n",
    "print(\"some words:\", voc_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2int = {token:idx for idx,token in enumerate(voc_list)}\n",
    "int_text = [[tokens2int[token.lower()] for token in text_l] for text_l in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4031"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2int[\"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['A',\n",
       "  'group',\n",
       "  'of',\n",
       "  '5',\n",
       "  'scuba',\n",
       "  'divers',\n",
       "  'talk',\n",
       "  'on',\n",
       "  'the',\n",
       "  'surface',\n",
       "  'next',\n",
       "  'to',\n",
       "  'a',\n",
       "  'barrier',\n",
       "  'island',\n",
       "  '.',\n",
       "  '\\n'],\n",
       " [1371,\n",
       "  3511,\n",
       "  3686,\n",
       "  2980,\n",
       "  4486,\n",
       "  1378,\n",
       "  222,\n",
       "  3911,\n",
       "  2286,\n",
       "  1341,\n",
       "  2715,\n",
       "  4255,\n",
       "  1371,\n",
       "  3260,\n",
       "  2247,\n",
       "  2213,\n",
       "  4031])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0], int_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knZtpGzZbKND"
   },
   "source": [
    "#### Exercise 4 - Create the reverse dictionaries for labels and tokens\n",
    "\n",
    "- create 2 dictionaries (int2label, int2token) to map integer labels and integer tokens back to labels and tokens \n",
    "\n",
    "This will be useful to be able to inspect results later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reverse mappings\n",
    "int2tokens = {idx:token for token,idx in tokens2int.items()}\n",
    "int2label = {idx:label for label,idx in label2int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dX_xNnPbKNJ"
   },
   "source": [
    "##### Pytorch import and key constants (PROVIDED)\n",
    "\n",
    "- `max_len` is the maximum sentence length (set to this to the size of the longest sequence in the _f30kE-captions-bio.txt_ corpus)\n",
    "- `batch_size` is the batch size\n",
    "- `embed_size` is the size of the pre-trained embeddings (word vectors). We use fasttext pre-trained embeddings of size 300.\n",
    "- `hidden_size` is the size of the RNN hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgHjYS72bKNP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length set at: 17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "max_len = max([len(l) for l in labels])\n",
    "print('Max Length set at:', max_len)\n",
    "batch_size = 64\n",
    "embed_size = 300\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5  - Creating tensors\n",
    "\n",
    "To work with pytorch, all data must be converted to tensors. Do the following\n",
    "\n",
    "* Create **X** and **Y**, which are tensors of dtype \"long\" that is initialised with 0s. It should be of the following size: (number of sentences, max sentence length) (cf. CS_pytorch)\n",
    "* We populate the zeros tensors with the input data from exercise 1.1: i.e.\n",
    "> For each element of \"texts\" and labels\", compute the length and if they are < max_length, make sure to pad them with \\<eos> to max_length \n",
    "\n",
    "**Hint:** You did this in the previous lab session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "X = torch.zeros((len(int_text), max_len), dtype=torch.long)\n",
    "Y = torch.zeros((len(int_text), max_len), dtype=torch.long)\n",
    "\n",
    "# Populate tensors with data\n",
    "for i, (int_text_l, int_labels_l) in enumerate(zip(int_text, int_labels)):\n",
    "    X[i, :len(int_text_l)] = torch.LongTensor(int_text_l)\n",
    "    Y[i, :len(int_labels_l)] = torch.LongTensor(int_labels_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1371, 3511, 3686,  ..., 2247, 2213, 4031],\n",
       "        [2731,  209, 1777,  ...,    0,    0,    0],\n",
       "        [1371, 3715, 3456,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1371, 2791, 3217,  ..., 2213, 4031,    0],\n",
       "        [1371, 2700, 3308,  ..., 2058, 2213, 4031],\n",
       "        [4031,    0,    0,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 2,  ..., 2, 0, 3],\n",
       "        [1, 0, 1,  ..., 0, 0, 0],\n",
       "        [1, 2, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 2, 2,  ..., 0, 3, 0],\n",
       "        [1, 2, 0,  ..., 0, 0, 3],\n",
       "        [3, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  6 - Create train and validation data\n",
    "\n",
    "* Split X into two parts, one called X_train which consists of the first 5000 items and the other called X_valid which includes the rest of the data\n",
    "* Do the same for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 17]) torch.Size([501, 17])\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:5000]\n",
    "Y_train = Y[:5000]\n",
    "\n",
    "X_valid = X[5000:]\n",
    "Y_valid = Y[5000:]\n",
    "\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1duVtHubKNk"
   },
   "source": [
    "#### Exercise  7 - Use torch DataLoader to split training and validation data into batches\n",
    "\n",
    "**Hint:** This was provided in the previous lab sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
    "# Note that input_features and labels must match on the length of the first dimension\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_9dJoanbKNp",
    "tags": []
   },
   "source": [
    "## Using pre-trained Fasttext embeddings\n",
    "\n",
    "Instead of learning word embeddings using the network, we will use pre-trained Fasttext embeddings. These are available [here](https://fasttext.cc/docs/en/english-vectors.html).\n",
    "\n",
    "However the pre-trained Fasttext embeddings cover several millions words and the full file is large. Instead we'll learn to extract a smaller subset that is restricted to the corpus vocabulary. Each line in that file contains a token followed by the Fasttext embedding of that token(300 dimensions). \n",
    "> e.g., auditorium -0.054196 -0.37375 ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8 (PROVIDED)\n",
    "\n",
    "* read the .vec file holding the FastText embeddings. Handle each line (a word and associated vector)\n",
    "* extract the set of vectors corresponding to the vocabulary of the _f30kE-captions-bio.txt_ corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4595, 4596\n"
     ]
    }
   ],
   "source": [
    "# from the FastText site, with amendments to check for vocab_set (set for faster check)\n",
    "def load_vectors(fname, vocab_set):\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] not in vocab_set: continue\n",
    "        # map each element (str) to float. \n",
    "        # set to list type (else it remain a map generator, and can be spent once called)\n",
    "        # avoid situation where students rerun cell and throws an error (after generator spent)\n",
    "        data[tokens[0]] = list(map(float, tokens[1:])) \n",
    "    return data\n",
    "\n",
    "# using https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "ft_vec_dict = load_vectors('wiki.en.filtered.vec', set(tokens2int))\n",
    "print(f'{len(ft_vec_dict)}, {len(tokens2int)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_9dJoanbKNp"
   },
   "source": [
    "#### Exercise 9: Associating pretrained vectors to our vocabulary\n",
    "To use the pre-trained embeddings, we do the following:\n",
    "\n",
    "* Firstly, we create a zeros  tensor of size (vocab_size, embedding_size) with values 0 (use torch.zeros method). The embedding_size should match the dimensions of the FastText embeddings. \n",
    "\n",
    "* Secondly, for each word in the _f30kE-captions-bio.txt_ corpus vocabulary, retrieve the FastText embedding from *ft_vec_dict* above.\n",
    "\n",
    "    * If the word is in our vocabulary we set the corresponding index (use your token2int dictionary) in our zeros tensor to the corresponding fasttext embedding. \n",
    "    * **NOTE**: If the word in our vocabulary is **not** available in the set of vectors, we set it to a vector of random values (use torch.rand_like()). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens2Vec = torch.zeros((len(voc_list), embed_size), dtype=torch.float)\n",
    "\n",
    "for i, token in enumerate(voc_list):\n",
    "\n",
    "    # if the token has a fasttext vector, use it\n",
    "    if token in ft_vec_dict:\n",
    "        Tokens2Vec[i, :] = torch.FloatTensor(ft_vec_dict[token])\n",
    "\n",
    "    # else, create a random vector\n",
    "    else:\n",
    "        Tokens2Vec[i, :] = torch.randn(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3295, -0.3003,  0.1981,  ...,  0.1219,  0.0210, -0.0637],\n",
       "        [-0.4088,  0.1644, -0.0666,  ...,  0.5124,  0.1331,  0.0500],\n",
       "        [ 0.3054, -0.2421, -0.4033,  ...,  0.5293,  0.3725,  0.6178],\n",
       "        ...,\n",
       "        [ 0.0280,  0.3455, -0.4234,  ...,  0.1321,  0.7562,  0.2801],\n",
       "        [-0.0907, -0.4747,  0.0026,  ...,  0.4564,  0.1347, -0.0868],\n",
       "        [-0.1047,  0.1041,  0.0323,  ...,  0.3182, -0.3078,  0.4265]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokens2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, train and evaluate your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  10 - Define your neural network (TODO: Provide missing values indicated by ??)\n",
    "\n",
    "As in the preceding Exercise sheet on neural classification, we define our RNN network as a subclass of pytorch RNN module. \n",
    "\n",
    "Our RNN consist of three layers:\n",
    "* the embedding layer: wich maps each token in the input to its fasttext embedding\n",
    "* A GRU layer: the recurrent layers\n",
    "* A decision layer which maps each input token to a label\n",
    "\n",
    "##### Padding\n",
    "If the input sentence is shorter than the maximum length, the remaining positions are filled with 0, the integer associated with the padding symbol. To exclude padding symbols  from the learning process (they are uninformative), include the padding_idx=vocab['<eos>'] option in the definition of the embedding layer and the \n",
    "\"bias=False\" option in the definition of the GRU layer. This forces the GRU hidden state to be null for all padding symbols. \n",
    "    \n",
    "##### Pre-trained Embeddings\n",
    "To ensure that the pretrained word embeddings are used:\n",
    "* set the `weight` attribute of the embedding layer to the pretrained embeddings\n",
    "* Use `requires_grad=False` to freeze the embedding layer so that the fasttext embeddings are not modified during learning.   \n",
    "If you do not use this option the embeddings are fine tuned during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4596, 300])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokens2Vec.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "bbrhekbVbKN4",
    "outputId": "5fcdb589-e8f2-4a53-e113-541f9186a5cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(4596, 300, padding_idx=4031)\n",
       "  (rnn): GRU(300, 128, bias=False, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (decision): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, pretrained_weights, hidden_size, num_layers, bidirectional = False, \n",
    "                 padding_idx = tokens2int['\\n']):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(Tokens2Vec.size(0), pretrained_weights.size(1), \n",
    "                                  padding_idx= tokens2int['\\n'])\n",
    "        self.embed.weight = nn.Parameter(pretrained_weights, requires_grad= False)\n",
    "        embed_size = self.embed.weight.size(-1)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, bias=False, num_layers=1, \n",
    "                            bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.decision = nn.Linear(hidden_size * num_layers * num_directions, len(label2int))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        output, hidden = self.rnn(embed)\n",
    "        return self.decision(self.dropout(output))\n",
    "\n",
    "rnn_model = RNN(Tokens2Vec, hidden_size, num_layers = 1, padding_idx = tokens2int['\\n'])\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYepFTvcbKOF"
   },
   "source": [
    "#### Exercise 11: Define a function to evaluate the performance of the model (PROVIDED)\n",
    "\n",
    "* the CrossEntropyLoss takes as input 2D matrices of shape (batch_size * sequence_length, num_labels)\n",
    "* Scores shape is adjusted accordingly\n",
    "* References are reshaped to (batch_size * sequence_length).\n",
    "* the max used to compute predictions applies to the last dimension of the y_scores tensors\n",
    "* To ignore padding symbols when computing the score, we create a matrix \"mask\" which contains 1 for all non nul elements of the Y matrix and O otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bc_mfttznImx",
    "outputId": "67ea45dd-650c-4ee7-a19a-4adcf6cd1988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.022857621996226662, 0.12809674768110837)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perf(model, loader, pad_idx):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss, correct, num_loss, num_perf = 0, 0, 0, 0\n",
    "    for x, y in loader:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_scores = model(x)\n",
    "            # should be of shape (bsz, max_len, vocab_size)\n",
    "            \n",
    "            # reshape y to a long sequence instead of batches of sequences\n",
    "            target = y.view(y.size(0) * y.size(1)) # or y.view(1,-1).squeeze()\n",
    "            scores = y_scores.view(y.size(0) * y.size(1), -1)\n",
    "            \n",
    "            loss = criterion(scores, target)\n",
    "            \n",
    "            y_pred = torch.max(y_scores, -1)[1] # argmax on last dim (vocab)\n",
    "\n",
    "            mask = (y != pad_idx) # ignore pads in original target\n",
    "            \n",
    "            correct += torch.sum((y_pred.data == y) * mask)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_loss += len(y)\n",
    "            num_perf += torch.sum(mask).item()\n",
    "    return total_loss / num_loss, correct.item() / num_perf\n",
    "\n",
    "pad_idx = tokens2int['\\n']\n",
    "perf(rnn_model, valid_loader, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbPWKZO2bKOJ"
   },
   "source": [
    "#### Exercise 12 - Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs):\n",
    "\n",
    "    # define the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # define the optimiser\n",
    "    optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # iterate over epochs    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # set the model in training mode\n",
    "        model.train()\n",
    "        # initialize the total_loss to 0\n",
    "        total_loss = 0\n",
    "\n",
    "        # iterate over batches\n",
    "        for x, y in train_loader:\n",
    "                \n",
    "            # reset the gradients\n",
    "            optimiser.zero_grad()\n",
    "            # predict the batch scores\n",
    "            y_scores = model(x)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(y_scores.transpose(1,2), y)\n",
    "\n",
    "            # Compute the gradients (backpropagation)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights (optimization)\n",
    "            optimiser.step()\n",
    "\n",
    "            # Update the batch loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"Epoch: \", epoch, \", Total loss: \", total_loss)\n",
    "        print(\" - Train: \", perf(model, train_loader, pad_idx))\n",
    "        print(\" - Validation: \", perf(model, valid_loader, pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , Total loss:  7.406806465238333\n",
      " - Train:  (0.0009451055426150561, 0.9806705882352941)\n",
      " - Validation:  (0.001241931539333747, 0.9748737818480686)\n",
      "Epoch:  1 , Total loss:  4.3728285119868815\n",
      " - Train:  (0.0007108289811760187, 0.9857764705882353)\n",
      " - Validation:  (0.0010171270968314416, 0.9789832100504873)\n",
      "Epoch:  2 , Total loss:  3.602778360247612\n",
      " - Train:  (0.0006021221715956926, 0.9882)\n",
      " - Validation:  (0.0009320896572457578, 0.9820359281437125)\n",
      "Epoch:  3 , Total loss:  3.225078333169222\n",
      " - Train:  (0.0005310760729014874, 0.9893176470588235)\n",
      " - Validation:  (0.000893633507801863, 0.9825055770811318)\n",
      "Epoch:  4 , Total loss:  2.86615295894444\n",
      " - Train:  (0.0004869469454512, 0.9907294117647059)\n",
      " - Validation:  (0.0008609373412446348, 0.9830926382529059)\n",
      "Epoch:  5 , Total loss:  2.603946018964052\n",
      " - Train:  (0.00043190165236592294, 0.9919764705882353)\n",
      " - Validation:  (0.0008579653923858902, 0.9834448749559704)\n",
      "Epoch:  6 , Total loss:  2.344331029802561\n",
      " - Train:  (0.00040228365026414393, 0.9925176470588235)\n",
      " - Validation:  (0.000847121927433623, 0.9832100504872607)\n",
      "Epoch:  7 , Total loss:  2.22994842287153\n",
      " - Train:  (0.00036242142030969263, 0.9932352941176471)\n",
      " - Validation:  (0.0008755693818161826, 0.9825055770811318)\n",
      "Epoch:  8 , Total loss:  2.0646579321473837\n",
      " - Train:  (0.00032180886082351206, 0.9944823529411765)\n",
      " - Validation:  (0.0008556434509164083, 0.9848538217682282)\n",
      "Epoch:  9 , Total loss:  1.7697766688652337\n",
      " - Train:  (0.0003016670702956617, 0.9946941176470588)\n",
      " - Validation:  (0.0008894444671933522, 0.9841493483620993)\n"
     ]
    }
   ],
   "source": [
    "# train the model for 10 epochs\n",
    "fit(rnn_model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply your model to a sentence\n",
    "\n",
    "Accuracy scores might be deceiving. We also need to look at the predictions on some example sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13: Apply your model to a sentence\n",
    "\n",
    "We define a `tag_sentence` function which:\n",
    "\n",
    "* takes a input the learned model and a sentence identifier i\n",
    "* retrieves from the data tensor X_valid the tensor for the i-th sentence (call it \"sentence\")\n",
    "* retrieves from the data tensor Y_valid the tensor of labels for the i-th sentence \n",
    "* put the model into evaluation mode\n",
    "* execute the model on the sentence tensor (\"sentence\")\n",
    "* extract the top predictions (use argmax)\n",
    "* print out the list of predicted tags \n",
    "   - use t.item() to get a value out of a tensor\n",
    "   - use your dictionary int2label (cf. Exercise 4 above) to print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN YOUR SOLUTION HERE ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_tagging-crf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
