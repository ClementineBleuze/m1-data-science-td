{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Make sure to install SpaCy and to load the english model: https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy doc objects\n",
    "- Applying a Spacy model to a string creates a Doc object\n",
    "- The Doc object is a sequence of tokens and all their linguistic annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load English Model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'Henri Poincaré sleeps. He smiles.'\n",
    "# Run SPaCy pipeline\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Henri Poincaré sleeps., He smiles.]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Get the Sentences\n",
    "[s for s in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Henri, Poincaré, sleeps, ., He, smiles, .]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Tokens\n",
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Henri, 'NNP', 'PROPN'), (Poincaré, 'NNP', 'PROPN'), (sleeps, 'VBZ', 'VERB')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Get Token tags and POS tags\n",
    "[(token, token.tag_, token.pos_) for token in doc[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'nsubj', is),\n",
       " ('is', 'ROOT', is),\n",
       " ('a', 'det', sentence),\n",
       " ('sentence', 'attr', is),\n",
       " ('.', 'punct', is),\n",
       " ('This', 'nsubj', is),\n",
       " ('is', 'ROOT', is),\n",
       " ('another', 'det', sentence),\n",
       " ('sentence', 'attr', is),\n",
       " ('about', 'prep', sentence),\n",
       " ('Henri', 'compound', Poincaré),\n",
       " ('Poincaré', 'pobj', about)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Syntactic Dependencies\n",
    "[(token.text, token.dep_, token.head) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Henri Poincaré]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Named Entities\n",
    "[ne for ne in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting a string into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load English Model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'This is a sentence. This is another sentence'\n",
    "# Run SPaCy pipeline\n",
    "sp_text = nlp(text)\n",
    "# Segment into sentences\n",
    "for sentence in sp_text.sents:\n",
    "   print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 0\n",
      "is 5\n",
      "a 8\n",
      "sentence 10\n",
      ". 18\n",
      "This 20\n",
      "is 25\n",
      "another 28\n",
      "sentence 36\n"
     ]
    }
   ],
   "source": [
    "for tok in sp_text:\n",
    "    print(tok,tok.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing Spacy output to a file**\n",
    "\n",
    "The `__repr__()` method is used to represent a class object (here: sentence) as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"out.txt\", \"w\")\n",
    "sentences = [sentence.__repr__() for sentence in sp_text.sents]\n",
    "print(\"SENTENCES\\n\\n\",\"\\n\".join(sentences), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting a string into tokens (Tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twenty\n",
      "-\n",
      "two\n",
      "years\n",
      "after\n",
      "the\n",
      "original\n",
      "Jurassic\n",
      "Park\n",
      "failed\n",
      ",\n",
      "the\n",
      "new\n",
      "park\n",
      ",\n",
      "also\n",
      "known\n",
      "as\n",
      "Jurassic\n",
      "World\n",
      ",\n",
      "is\n",
      "open\n",
      "for\n",
      "business\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load English Model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Twenty-two years after the original Jurassic Park failed, the new park, also known as Jurassic World, is open for business.\"\n",
    "# Run SpaCy pipeline\n",
    "sp_text = nlp(text)\n",
    "# Get tokens\n",
    "for tok in sp_text:\n",
    "   print(tok.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twenty-two years after the original Jurassic Park failed, the new park, also known as Jurassic World, is open for business."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twenty\n",
      "two\n",
      "after\n",
      "the\n",
      "the\n",
      "also\n",
      "as\n",
      "is\n",
      "for\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load English Model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Twenty-two years after the original Jurassic Park failed, the new park, also known as Jurassic World, is open for business.\"\n",
    "# Run SpaCy pipeline\n",
    "sp_text = nlp(text)\n",
    "for token in sp_text:\n",
    "   if token.is_stop == True:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Amélie, 'NNP', 'PROPN'),\n",
       " (is, 'VBZ', 'AUX'),\n",
       " (a, 'DT', 'DET'),\n",
       " (story, 'NN', 'NOUN'),\n",
       " (about, 'IN', 'ADP'),\n",
       " (a, 'DT', 'DET'),\n",
       " (girl, 'NN', 'NOUN'),\n",
       " (named, 'VBN', 'VERB'),\n",
       " (Amélie, 'NNP', 'PROPN'),\n",
       " (whose, 'WP$', 'DET'),\n",
       " (childhood, 'NN', 'NOUN'),\n",
       " (was, 'VBD', 'AUX'),\n",
       " (suppressed, 'VBN', 'VERB'),\n",
       " (by, 'IN', 'ADP'),\n",
       " (her, 'PRP$', 'DET'),\n",
       " (Father, 'NNP', 'PROPN'),\n",
       " ('s, 'POS', 'PART'),\n",
       " (mistaken, 'JJ', 'ADJ'),\n",
       " (concerns, 'NNS', 'NOUN'),\n",
       " (of, 'IN', 'ADP'),\n",
       " (a, 'DT', 'DET'),\n",
       " (heart, 'NN', 'NOUN'),\n",
       " (defect, 'NN', 'NOUN'),\n",
       " (., '.', 'PUNCT')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the SpaCy model for English\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define test sentence\n",
    "sentence = \"Amélie is a story about a girl named Amélie whose childhood was suppressed by her Father's mistaken concerns of a heart defect.\"\n",
    "\n",
    "# Apply the SpaCy model to the text\n",
    "nlp_sentence = nlp(sentence)\n",
    "\n",
    "# For each token in the text, retrieve this token its tag and its part of speech\n",
    "spacy_pos_tagged = [(token, token.tag_, token.pos_) for token in nlp_sentence]\n",
    "spacy_pos_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suppress'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "word_form = \"suppressed\"\n",
    "# Apply the SpaCy  model to the input\n",
    "# The result is a list\n",
    "token = nlp(word_form)\n",
    "\n",
    "# Extract the lemma for the first token in the list returned by the previous statement\n",
    "token[0].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj bought\n",
      "bought buy VERB VBD ROOT bought\n",
      "U.K. U.K. PROPN NNP compound startup\n",
      "startup startup NOUN NN dobj bought\n",
      "for for ADP IN prep bought\n",
      "$ $ SYM $ quantmod billion\n",
      "1 1 NUM CD compound billion\n",
      "billion billion NUM CD pobj for\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple bought U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEs: [Amélie]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Amélie is a story about a girl named \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amélie\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " whose childhood was suppressed by her Father's mistaken concerns of a heart defect.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pprint import pprint\n",
    "nlp = spacy.load('en_core_web_sm', entity=True)\n",
    "doc = nlp(\"Amélie is a story about a girl named Amélie whose childhood was suppressed by her Father's mistaken concerns of a heart defect.\")\n",
    "\n",
    "\n",
    "print(\"NEs:\", [ne for ne in doc.ents])\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
