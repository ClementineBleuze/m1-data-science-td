{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7N-x97sa7h4"
      },
      "source": [
        "# Exercise \"Lecture 14: Neural Classification\"\n",
        "\n",
        "\n",
        "In this set of exercises, we will use a Recurrent Neural Network to classify BBC news articles into 5 topics. The dataset consists of 2225 documents and 5 categories: business, entertainment, politics, sport, and technology. \n",
        "\n",
        "\n",
        "The exercises cover the following points:\n",
        "\n",
        "* Converting the text in the corpus to vectors of integers (each integer represents a word in the corpus vocabulary)\n",
        "* Computing some descriptive statistics to identify a sentence length cutoff (sentences with longer lengths will not be considered for training)\n",
        "* Specifying, training and testing a recurrent neural network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxQpkNWYCFZn",
        "outputId": "9bc3da62-ca07-4bef-bbc3-e10a8b461068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqYTwbCYCCfq",
        "tags": []
      },
      "source": [
        "### Preprocessing (PROVIDED)\n",
        "\n",
        "We first prepocessed the data to extract X (the input) and Y (the input labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IbfyN3fSCCft",
        "outputId": "facff414-6af0-4ac6-9bcc-bbdf52d7719c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fb438c64-fc3c-444b-969f-098f267449dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_report</th>\n",
              "      <th>labels</th>\n",
              "      <th>labels_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tate &amp; Lyle boss bags top award\\n\\nTate &amp; Lyle...</td>\n",
              "      <td>0</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Halo 2 sells five million copies\\n\\nMicrosoft ...</td>\n",
              "      <td>4</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MSPs hear renewed climate warning\\n\\nClimate c...</td>\n",
              "      <td>2</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pavey focuses on indoor success\\n\\nJo Pavey wi...</td>\n",
              "      <td>3</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tories reject rethink on axed MP\\n\\nSacked MP ...</td>\n",
              "      <td>2</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb438c64-fc3c-444b-969f-098f267449dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb438c64-fc3c-444b-969f-098f267449dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb438c64-fc3c-444b-969f-098f267449dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                         news_report  labels labels_text\n",
              "0  Tate & Lyle boss bags top award\\n\\nTate & Lyle...       0    business\n",
              "1  Halo 2 sells five million copies\\n\\nMicrosoft ...       4        tech\n",
              "2  MSPs hear renewed climate warning\\n\\nClimate c...       2    politics\n",
              "3  Pavey focuses on indoor success\\n\\nJo Pavey wi...       3       sport\n",
              "4  Tories reject rethink on axed MP\\n\\nSacked MP ...       2    politics"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "\n",
        "# for reproducibility\n",
        "random_state = 0 \n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/NLP Masters/M2/Data Science/Data Science Labs/14/bbc/\"\n",
        "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\", random_state=random_state)\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'news_report': data['data'], 'labels': data['target']})\n",
        "# add text labels \n",
        "df['labels_text'] = df.labels.apply(lambda x: data['target_names'][x])\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchG9cxna7jE"
      },
      "source": [
        "Extracting X (texts) and Y (classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjVnCCXNa7jJ",
        "outputId": "723c284e-446f-45cd-bcbc-76a89abb8645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spain coach faces racism inquiry\n",
            "\n",
            "Spain's Football Federation has initiated disciplinary action against national coach Luis Aragones over racist comments about Thierry Henry.\n",
            "\n",
            "If found guilty Aragones could lose his job or face a fine of about Â£22,000. The federation had initially declined to take action against Aragones after comments he made during a national team training session in October. But its president Angel Maria Villar changed his mind after a request by Spain's anti-violence commission. Aragones insisted the comments, made to Henry's Arsenal club-mate Jose Antonio Reyes, were meant to motivate the player, and were not intended to be offensive.\n",
            "\n",
            "\"I never intended to offend anyone, and for that reason I have a very easy conscience,\" he said at the time. \"I'm obliged to motivate my players to get the best results. \"As part of that job, I use colloquial language, with which we can all understand each other within the framework of the football world. \" England's players made a point of wearing anti-racism t-shirts when training before their friendly against Spain in Madrid last month.\n",
            "\n",
            "But the storm increased following racist chanting by Spanish fans at England's black players during the game, which Spain won 1-0. Spain's minister of sport Jaime Lissavetzky was quick to give his backing to the Federation's decision. \"Everyone who has a public function has to consider their declarations, and make sure they do not give a negative image,\" he said. \"We are going to have zero tolerance in questions of racism.\"\n",
            "\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "texts = df[\"news_report\"]\n",
        "labels = df[\"labels\"]\n",
        "\n",
        "n=100\n",
        "print(texts[n])\n",
        "print(labels[n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Mqk-2NCCf2"
      },
      "source": [
        "### 2 Converting the texts in the corpus to vectors of integers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYbQI1mka7kl"
      },
      "source": [
        "#### Exercise 1 -  Convert the corpus to a list of lists of integers\n",
        "\n",
        "* Define a dictionary tokens2int which maps each distinct token in the corpus to a distinct integer \n",
        "(the size of this dictionary is the size of the corpus vocabulary i.e., the number of distinct tokens in the corpus, cf. Python CS)\n",
        "* Use this dictionary to map each news report to a vector.\n",
        "\n",
        "\n",
        "**Example**\n",
        "* Input Texts: [\"The woman put the book on the table\", \"The woman reads\"]\n",
        "* Created vocabulary: {the, woman, put, book, on, table, reads}\n",
        "* Output Texts: [ [1,2,3,1,4,5,1,6], [1,2,7]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wGhfmST3Dmx",
        "outputId": "930f6de5-247b-48e4-d3a3-6aca65d0e09c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax6WQpudCCf5",
        "outputId": "faae131f-d479-4fe5-fa20-08aefa73afcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'tate': 1,\n",
              "             '&': 2,\n",
              "             'lyle': 3,\n",
              "             'boss': 4,\n",
              "             'bags': 5,\n",
              "             'top': 6,\n",
              "             'award': 7,\n",
              "             \"'s\": 8,\n",
              "             'chief': 9,\n",
              "             'executive': 10,\n",
              "             'has': 11,\n",
              "             'been': 12,\n",
              "             'named': 13,\n",
              "             'european': 14,\n",
              "             'businessman': 15,\n",
              "             'of': 16,\n",
              "             'the': 17,\n",
              "             'year': 18,\n",
              "             'by': 19,\n",
              "             'a': 20,\n",
              "             'leading': 21,\n",
              "             'business': 22,\n",
              "             'magazine': 23,\n",
              "             '.': 24,\n",
              "             'iain': 25,\n",
              "             'ferguson': 26,\n",
              "             'was': 27,\n",
              "             'awarded': 28,\n",
              "             'title': 29,\n",
              "             'us': 30,\n",
              "             'publication': 31,\n",
              "             'forbes': 32,\n",
              "             'for': 33,\n",
              "             'returning': 34,\n",
              "             'one': 35,\n",
              "             'uk': 36,\n",
              "             '``': 37,\n",
              "             'venerable': 38,\n",
              "             \"''\": 39,\n",
              "             'manufacturers': 40,\n",
              "             'to': 41,\n",
              "             'country': 42,\n",
              "             '100': 43,\n",
              "             'companies': 44,\n",
              "             'sugar': 45,\n",
              "             'group': 46,\n",
              "             'had': 47,\n",
              "             'absent': 48,\n",
              "             'from': 49,\n",
              "             'ftse': 50,\n",
              "             'seven': 51,\n",
              "             'years': 52,\n",
              "             'until': 53,\n",
              "             'mr': 54,\n",
              "             'helped': 55,\n",
              "             'it': 56,\n",
              "             'return': 57,\n",
              "             'growth': 58,\n",
              "             'shares': 59,\n",
              "             'have': 60,\n",
              "             'leapt': 61,\n",
              "             '55': 62,\n",
              "             '%': 63,\n",
              "             'this': 64,\n",
              "             ',': 65,\n",
              "             'boosted': 66,\n",
              "             'firming': 67,\n",
              "             'prices': 68,\n",
              "             'and': 69,\n",
              "             'sales': 70,\n",
              "             'its': 71,\n",
              "             'artificial': 72,\n",
              "             'sweeteners': 73,\n",
              "             'after': 74,\n",
              "             'sagging': 75,\n",
              "             'stock': 76,\n",
              "             'price': 77,\n",
              "             'seven-year': 78,\n",
              "             'hiatus': 79,\n",
              "             'britain': 80,\n",
              "             'returned': 81,\n",
              "             'vaunted': 82,\n",
              "             'index': 83,\n",
              "             'said': 84,\n",
              "             'took': 85,\n",
              "             'helm': 86,\n",
              "             'at': 87,\n",
              "             'company': 88,\n",
              "             'in': 89,\n",
              "             '2003': 90,\n",
              "             'spending': 91,\n",
              "             'most': 92,\n",
              "             'his': 93,\n",
              "             'career': 94,\n",
              "             'consumer': 95,\n",
              "             'goods': 96,\n",
              "             'giant': 97,\n",
              "             'unilever': 98,\n",
              "             'which': 99,\n",
              "             'an': 100,\n",
              "             'original': 101,\n",
              "             'member': 102,\n",
              "             'historic': 103,\n",
              "             'ft-30': 104,\n",
              "             '1935': 105,\n",
              "             'operates': 106,\n",
              "             'more': 107,\n",
              "             'than': 108,\n",
              "             '41': 109,\n",
              "             'factories': 110,\n",
              "             '20': 111,\n",
              "             'additional': 112,\n",
              "             'production': 113,\n",
              "             'facilities': 114,\n",
              "             '28': 115,\n",
              "             'countries': 116,\n",
              "             'previous': 117,\n",
              "             'winners': 118,\n",
              "             'include': 119,\n",
              "             'royal': 120,\n",
              "             'bank': 121,\n",
              "             'scotland': 122,\n",
              "             'fred': 123,\n",
              "             'goodwin': 124,\n",
              "             'former': 125,\n",
              "             'vodafone': 126,\n",
              "             'chris': 127,\n",
              "             'gent': 128,\n",
              "             'halo': 129,\n",
              "             '2': 130,\n",
              "             'sells': 131,\n",
              "             'five': 132,\n",
              "             'million': 133,\n",
              "             'copies': 134,\n",
              "             'microsoft': 135,\n",
              "             'is': 136,\n",
              "             'celebrating': 137,\n",
              "             'bumper': 138,\n",
              "             'xbox': 139,\n",
              "             'sci-fi': 140,\n",
              "             'shooter': 141,\n",
              "             'game': 142,\n",
              "             'sold': 143,\n",
              "             'worldwide': 144,\n",
              "             'since': 145,\n",
              "             'went': 146,\n",
              "             'on': 147,\n",
              "             'sale': 148,\n",
              "             'mid-november': 149,\n",
              "             'proved': 150,\n",
              "             'popular': 151,\n",
              "             'online': 152,\n",
              "             'with': 153,\n",
              "             'gamers': 154,\n",
              "             'notching': 155,\n",
              "             'up': 156,\n",
              "             'record': 157,\n",
              "             'hours': 158,\n",
              "             'playing': 159,\n",
              "             'live': 160,\n",
              "             'according': 161,\n",
              "             'nine': 162,\n",
              "             'out': 163,\n",
              "             '10': 164,\n",
              "             'members': 165,\n",
              "             'played': 166,\n",
              "             'average': 167,\n",
              "             '91': 168,\n",
              "             'minutes': 169,\n",
              "             'per': 170,\n",
              "             'session': 171,\n",
              "             'sequel': 172,\n",
              "             'best-selling': 173,\n",
              "             'need': 174,\n",
              "             'speed': 175,\n",
              "             ':': 176,\n",
              "             'underground': 177,\n",
              "             'inched': 178,\n",
              "             'ahead': 179,\n",
              "             'competition': 180,\n",
              "             'take': 181,\n",
              "             'slot': 182,\n",
              "             'official': 183,\n",
              "             'games': 184,\n",
              "             'charts': 185,\n",
              "             'racing': 186,\n",
              "             'moved': 187,\n",
              "             'spot': 188,\n",
              "             'first': 189,\n",
              "             'place': 190,\n",
              "             'nudging': 191,\n",
              "             'gta': 192,\n",
              "             'san': 193,\n",
              "             'andreas': 194,\n",
              "             'down': 195,\n",
              "             'second': 196,\n",
              "             'dropped': 197,\n",
              "             'while': 198,\n",
              "             'half-life': 199,\n",
              "             'fell': 200,\n",
              "             'number': 201,\n",
              "             'last': 202,\n",
              "             'week': 203,\n",
              "             'new': 204,\n",
              "             'releases': 205,\n",
              "             'goldeneye': 206,\n",
              "             'rogue': 207,\n",
              "             'agent': 208,\n",
              "             'killzone': 209,\n",
              "             'both': 210,\n",
              "             'failed': 211,\n",
              "             'make': 212,\n",
              "             'into': 213,\n",
              "             'debuting': 214,\n",
              "             '11': 215,\n",
              "             '12': 216,\n",
              "             'respectively': 217,\n",
              "             'numbers': 218,\n",
              "             'warcraft': 219,\n",
              "             'fans': 220,\n",
              "             'are': 221,\n",
              "             'settling': 222,\n",
              "             'world': 223,\n",
              "             'opening': 224,\n",
              "             'day': 225,\n",
              "             'massive': 226,\n",
              "             'multi-player': 227,\n",
              "             '200,000': 228,\n",
              "             'players': 229,\n",
              "             'signed': 230,\n",
              "             'play': 231,\n",
              "             'evening': 232,\n",
              "             '100,000': 233,\n",
              "             'were': 234,\n",
              "             'forcing': 235,\n",
              "             'blizzard': 236,\n",
              "             'add': 237,\n",
              "             'another': 238,\n",
              "             '34': 239,\n",
              "             'servers': 240,\n",
              "             'cope': 241,\n",
              "             'influx': 242,\n",
              "             'turns': 243,\n",
              "             'stand': 244,\n",
              "             'alone': 245,\n",
              "             'persistent': 246,\n",
              "             'that': 247,\n",
              "             'can': 248,\n",
              "             'inhabit': 249,\n",
              "             'not': 250,\n",
              "             'just': 251,\n",
              "             'visit': 252,\n",
              "             'europe': 253,\n",
              "             'could': 254,\n",
              "             'be': 255,\n",
              "             'waiting': 256,\n",
              "             'january': 257,\n",
              "             'hear': 258,\n",
              "             'when': 259,\n",
              "             'they': 260,\n",
              "             'get': 261,\n",
              "             'their': 262,\n",
              "             'mitts': 263,\n",
              "             'nintendo': 264,\n",
              "             'handheld': 265,\n",
              "             'device': 266,\n",
              "             'ds': 267,\n",
              "             'says': 268,\n",
              "             'gamesindustry.biz': 269,\n",
              "             'david': 270,\n",
              "             'yarnton': 271,\n",
              "             'general': 272,\n",
              "             'manager': 273,\n",
              "             'told': 274,\n",
              "             'press': 275,\n",
              "             'conference': 276,\n",
              "             'look': 277,\n",
              "             'details': 278,\n",
              "             'launch': 279,\n",
              "             'sunday': 280,\n",
              "             'goes': 281,\n",
              "             'japan': 282,\n",
              "             'december': 283,\n",
              "             '95': 284,\n",
              "             'share': 285,\n",
              "             'gaming': 286,\n",
              "             'market': 287,\n",
              "             'expected': 288,\n",
              "             'sell': 289,\n",
              "             'around': 290,\n",
              "             'march': 291,\n",
              "             '2005': 292,\n",
              "             'msps': 293,\n",
              "             'renewed': 294,\n",
              "             'climate': 295,\n",
              "             'warning': 296,\n",
              "             'change': 297,\n",
              "             'completely': 298,\n",
              "             'control': 299,\n",
              "             'within': 300,\n",
              "             'several': 301,\n",
              "             'decades': 302,\n",
              "             'scottish': 303,\n",
              "             'environment': 304,\n",
              "             'protection': 305,\n",
              "             'agency': 306,\n",
              "             'committee': 307,\n",
              "             'experts': 308,\n",
              "             'giving': 309,\n",
              "             'evidence': 310,\n",
              "             'subject': 311,\n",
              "             'parliament': 312,\n",
              "             'officials': 313,\n",
              "             'believe': 314,\n",
              "             'nuclear': 315,\n",
              "             'energy': 316,\n",
              "             'wind': 317,\n",
              "             'farms': 318,\n",
              "             'may': 319,\n",
              "             'better': 320,\n",
              "             'options': 321,\n",
              "             'trying': 322,\n",
              "             'tackle': 323,\n",
              "             'global': 324,\n",
              "             'warming': 325,\n",
              "             'solutions': 326,\n",
              "             'suggested': 327,\n",
              "             'conservationists': 328,\n",
              "             'reducing': 329,\n",
              "             'internal': 330,\n",
              "             'air': 331,\n",
              "             'travel': 332,\n",
              "             'boosting': 333,\n",
              "             'electric': 334,\n",
              "             'trains': 335,\n",
              "             'part': 336,\n",
              "             'inquiry': 337,\n",
              "             'impact': 338,\n",
              "             'sepa': 339,\n",
              "             'attempting': 340,\n",
              "             'curb': 341,\n",
              "             'gases': 342,\n",
              "             'as': 343,\n",
              "             'pollution': 344,\n",
              "             'transport': 345,\n",
              "             'emissions': 346,\n",
              "             'increases': 347,\n",
              "             'ecologists': 348,\n",
              "             'accept': 349,\n",
              "             'significant': 350,\n",
              "             'intrusion': 351,\n",
              "             'likely': 352,\n",
              "             'also': 353,\n",
              "             'power': 354,\n",
              "             'will': 355,\n",
              "             'needed': 356,\n",
              "             'possibly': 357,\n",
              "             'predict': 358,\n",
              "             'two': 359,\n",
              "             'methods': 360,\n",
              "             'remain': 361,\n",
              "             'sources': 362,\n",
              "             'under': 363,\n",
              "             'studying': 364,\n",
              "             'seas': 365,\n",
              "             'off': 366,\n",
              "             'west': 367,\n",
              "             'coast': 368,\n",
              "             'already': 369,\n",
              "             'forecast': 370,\n",
              "             'devastating': 371,\n",
              "             'weather': 372,\n",
              "             'type': 373,\n",
              "             'caused': 374,\n",
              "             'havoc': 375,\n",
              "             'across': 376,\n",
              "             'month': 377,\n",
              "             'predicted': 378,\n",
              "             'damaging': 379,\n",
              "             'storms': 380,\n",
              "             'become': 381,\n",
              "             'frequent': 382,\n",
              "             'researchers': 383,\n",
              "             'university': 384,\n",
              "             'highlands': 385,\n",
              "             'islands': 386,\n",
              "             'southampton': 387,\n",
              "             'looking': 388,\n",
              "             'wave': 389,\n",
              "             'heights': 390,\n",
              "             'atlantic': 391,\n",
              "             'over': 392,\n",
              "             'project': 393,\n",
              "             'conducted': 394,\n",
              "             'jointly': 395,\n",
              "             'environmental': 396,\n",
              "             'research': 397,\n",
              "             'institute': 398,\n",
              "             'thurso': 399,\n",
              "             '(': 400,\n",
              "             'uhi': 401,\n",
              "             ')': 402,\n",
              "             'millennium': 403,\n",
              "             'network': 404,\n",
              "             'oceanography': 405,\n",
              "             'centre': 406,\n",
              "             'scientists': 407,\n",
              "             'carried': 408,\n",
              "             'series': 409,\n",
              "             'studies': 410,\n",
              "             'including': 411,\n",
              "             'use': 412,\n",
              "             'satellites': 413,\n",
              "             'assess': 414,\n",
              "             'hebrides': 415,\n",
              "             'pavey': 416,\n",
              "             'focuses': 417,\n",
              "             'indoor': 418,\n",
              "             'success': 419,\n",
              "             'jo': 420,\n",
              "             'miss': 421,\n",
              "             'view': 422,\n",
              "             'great': 423,\n",
              "             'edinburgh': 424,\n",
              "             'international': 425,\n",
              "             'cross': 426,\n",
              "             'focus': 427,\n",
              "             'preparing': 428,\n",
              "             'championships': 429,\n",
              "             '31-year-old': 430,\n",
              "             'third': 431,\n",
              "             'behind': 432,\n",
              "             'hayley': 433,\n",
              "             'yelling': 434,\n",
              "             'justyna': 435,\n",
              "             'bak': 436,\n",
              "             'but': 437,\n",
              "             'she': 438,\n",
              "             'prefers': 439,\n",
              "             'race': 440,\n",
              "             'track': 441,\n",
              "             'winning': 442,\n",
              "             'bronze': 443,\n",
              "             'i': 444,\n",
              "             \"'m\": 445,\n",
              "             'wary': 446,\n",
              "             'injuries': 447,\n",
              "             'must': 448,\n",
              "             'concentrate': 449,\n",
              "             'season': 450,\n",
              "             'because': 451,\n",
              "             'do': 452,\n",
              "             \"n't\": 453,\n",
              "             'even': 454,\n",
              "             'run': 455,\n",
              "             'hills': 456,\n",
              "             'training': 457,\n",
              "             'who': 458,\n",
              "             'came': 459,\n",
              "             'fifth': 460,\n",
              "             '5,000m': 461,\n",
              "             'athens': 462,\n",
              "             'olympics': 463,\n",
              "             'british': 464,\n",
              "             'team': 465,\n",
              "             'win': 466,\n",
              "             'silver': 467,\n",
              "             'medal': 468,\n",
              "             'heringsdorf': 469,\n",
              "             'start': 470,\n",
              "             'her': 471,\n",
              "             '3,000m': 472,\n",
              "             'either': 473,\n",
              "             'boston': 474,\n",
              "             'or': 475,\n",
              "             'stuttgart': 476,\n",
              "             'end': 477,\n",
              "             'tories': 478,\n",
              "             'reject': 479,\n",
              "             'rethink': 480,\n",
              "             'axed': 481,\n",
              "             'mp': 482,\n",
              "             'sacked': 483,\n",
              "             'howard': 484,\n",
              "             'flight': 485,\n",
              "             'local': 486,\n",
              "             'conservative': 487,\n",
              "             'association': 488,\n",
              "             'insisted': 489,\n",
              "             'he': 490,\n",
              "             'candidate': 491,\n",
              "             'election': 492,\n",
              "             'russell': 493,\n",
              "             'tanguay': 494,\n",
              "             'arundel': 495,\n",
              "             'south': 496,\n",
              "             'downs': 497,\n",
              "             'ineligible': 498,\n",
              "             'seeking': 499,\n",
              "             'substitute': 500,\n",
              "             'news': 501,\n",
              "             'comes': 502,\n",
              "             'despite': 503,\n",
              "             'allies': 504,\n",
              "             'saying': 505,\n",
              "             'enough': 506,\n",
              "             'support': 507,\n",
              "             'hold': 508,\n",
              "             'meeting': 509,\n",
              "             'discuss': 510,\n",
              "             'fate': 511,\n",
              "             'landed': 512,\n",
              "             'trouble': 513,\n",
              "             'remarks': 514,\n",
              "             'tory': 515,\n",
              "             'tax': 516,\n",
              "             'plans': 517,\n",
              "             'quit': 518,\n",
              "             'deputy': 519,\n",
              "             'chairman': 520,\n",
              "             'apparently': 521,\n",
              "             'suggesting': 522,\n",
              "             'planned': 523,\n",
              "             'extra': 524,\n",
              "             'cuts': 525,\n",
              "             '-': 526,\n",
              "             'wants': 527,\n",
              "             'continue': 528,\n",
              "             'headquarters': 529,\n",
              "             'no': 530,\n",
              "             'longer': 531,\n",
              "             'approved': 532,\n",
              "             'backed': 533,\n",
              "             'tuesday': 534,\n",
              "             'party': 535,\n",
              "             'process': 536,\n",
              "             'selecting': 537,\n",
              "             'made': 538,\n",
              "             'similar': 539,\n",
              "             'comments': 540,\n",
              "             'friday': 541,\n",
              "             'dissent': 542,\n",
              "             'continues': 543,\n",
              "             'councillors': 544,\n",
              "             'back': 545,\n",
              "             'met': 546,\n",
              "             'afternoon': 547,\n",
              "             'did': 548,\n",
              "             'comment': 549,\n",
              "             'left': 550,\n",
              "             'unless': 551,\n",
              "             'instructs': 552,\n",
              "             'him': 553,\n",
              "             'so': 554,\n",
              "             'extraordinary': 555,\n",
              "             'egm': 556,\n",
              "             'consulting': 557,\n",
              "             'lawyers': 558,\n",
              "             'bbc': 559,\n",
              "             'selected': 560,\n",
              "             'me': 561,\n",
              "             'if': 562,\n",
              "             'you': 563,\n",
              "             'like': 564,\n",
              "             'dispose': 565,\n",
              "             'keep': 566,\n",
              "             'supporters': 567,\n",
              "             'say': 568,\n",
              "             '50': 569,\n",
              "             'signatures': 570,\n",
              "             'trigger': 571,\n",
              "             'rules': 572,\n",
              "             'leader': 573,\n",
              "             'argues': 574,\n",
              "             'ensuring': 575,\n",
              "             'honesty': 576,\n",
              "             'we': 577,\n",
              "             'thing': 578,\n",
              "             'private': 579,\n",
              "             'public': 580,\n",
              "             'labour': 581,\n",
              "             'campaign': 582,\n",
              "             'coordinator': 583,\n",
              "             'alan': 584,\n",
              "             'milburn': 585,\n",
              "             'turmoil': 586,\n",
              "             'exposed': 587,\n",
              "             'hidden': 588,\n",
              "             'one-off': 589,\n",
              "             'claiming': 590,\n",
              "             'other': 591,\n",
              "             'senior': 592,\n",
              "             'obsessively': 593,\n",
              "             'committed': 594,\n",
              "             'cutting': 595,\n",
              "             'liberal': 596,\n",
              "             'democrat': 597,\n",
              "             'matthew': 598,\n",
              "             'taylor': 599,\n",
              "             'whilst': 600,\n",
              "             'disagree': 601,\n",
              "             'views': 602,\n",
              "             'seems': 603,\n",
              "             'sack': 604,\n",
              "             'somebody': 605,\n",
              "             'telling': 606,\n",
              "             'truth': 607,\n",
              "             'emerged': 608,\n",
              "             'suspended': 609,\n",
              "             'slough': 610,\n",
              "             'constituency': 611,\n",
              "             'refusing': 612,\n",
              "             'deselect': 613,\n",
              "             'adrian': 614,\n",
              "             'hilton': 615,\n",
              "             'abandoned': 616,\n",
              "             'signing': 617,\n",
              "             'maastricht': 618,\n",
              "             'treaty': 619,\n",
              "             'john': 620,\n",
              "             'major': 621,\n",
              "             'government': 622,\n",
              "             'act': 623,\n",
              "             'treason': 624,\n",
              "             'catholic': 625,\n",
              "             'herald': 626,\n",
              "             'highlighted': 627,\n",
              "             'articles': 628,\n",
              "             'wrote': 629,\n",
              "             'about': 630,\n",
              "             'role': 631,\n",
              "             'catholicism': 632,\n",
              "             'union': 633,\n",
              "             'chosen': 634,\n",
              "             'fight': 635,\n",
              "             'seat': 636,\n",
              "             'robert': 637,\n",
              "             'oulds': 638,\n",
              "             'being': 639,\n",
              "             'pictured': 640,\n",
              "             'range': 641,\n",
              "             'guns': 642,\n",
              "             'hunting': 643,\n",
              "             'knife': 644,\n",
              "             'now': 645,\n",
              "             'placed': 646,\n",
              "             'status': 647,\n",
              "             'spokesman': 648,\n",
              "             'considering': 649,\n",
              "             'taking': 650,\n",
              "             'legal': 651,\n",
              "             'action': 652,\n",
              "             'against': 653,\n",
              "             'deposal': 654,\n",
              "             'only': 655,\n",
              "             'learned': 656,\n",
              "             'final': 657,\n",
              "             'decision': 658,\n",
              "             'website': 659,\n",
              "             'monday': 660,\n",
              "             'there': 661,\n",
              "             'people': 662,\n",
              "             'central': 663,\n",
              "             'office': 664,\n",
              "             'behaving': 665,\n",
              "             'little': 666,\n",
              "             'dictators': 667,\n",
              "             'seemingly': 668,\n",
              "             'ordinary': 669,\n",
              "             'treated': 670,\n",
              "             'contempt': 671,\n",
              "             'try': 672,\n",
              "             'contact': 673,\n",
              "             'lib': 674,\n",
              "             'dems': 675,\n",
              "             \"'best\": 676,\n",
              "             'ever': 677,\n",
              "             \"poll'\": 678,\n",
              "             'set': 679,\n",
              "             'best': 680,\n",
              "             'results': 681,\n",
              "             'council': 682,\n",
              "             'polls': 683,\n",
              "             'frontbenchers': 684,\n",
              "             'ed': 685,\n",
              "             'davey': 686,\n",
              "             'speaking': 687,\n",
              "             'launched': 688,\n",
              "             'elections': 689,\n",
              "             'held': 690,\n",
              "             '37': 691,\n",
              "             'english': 692,\n",
              "             'areas': 693,\n",
              "             'flagship': 694,\n",
              "             'pledge': 695,\n",
              "             'replace': 696,\n",
              "             'income': 697,\n",
              "             'would': 698,\n",
              "             'pay': 699,\n",
              "             'sums': 700,\n",
              "             'coming': 701,\n",
              "             'all': 702,\n",
              "             '5': 703,\n",
              "             'going': 704,\n",
              "             'votes': 705,\n",
              "             'seats': 706,\n",
              "             'think': 707,\n",
              "             '[': 708,\n",
              "             ']': 709,\n",
              "             'charles': 710,\n",
              "             'kennedy': 711,\n",
              "             'stronger': 712,\n",
              "             'endorsement': 713,\n",
              "             'attacks': 714,\n",
              "             \"'pay\": 715,\n",
              "             'later': 716,\n",
              "             \"'\": 717,\n",
              "             'budget': 718,\n",
              "             'michael': 719,\n",
              "             'dismissed': 720,\n",
              "             'gordon': 721,\n",
              "             'brown': 722,\n",
              "             'vote': 723,\n",
              "             'simple': 724,\n",
              "             'fact': 725,\n",
              "             'taxes': 726,\n",
              "             'go': 727,\n",
              "             'plug': 728,\n",
              "             'financial': 729,\n",
              "             'black': 730,\n",
              "             'hole': 731,\n",
              "             'everyone': 732,\n",
              "             'see': 733,\n",
              "             'chancellor': 734,\n",
              "             'these': 735,\n",
              "             'hid': 736,\n",
              "             'rises': 737,\n",
              "             'hard': 738,\n",
              "             'working': 739,\n",
              "             'families': 740,\n",
              "             'faltering': 741,\n",
              "             'package': 742,\n",
              "             'measures': 743,\n",
              "             'added': 744,\n",
              "             'what': 745,\n",
              "             'good': 746,\n",
              "             'interests': 747,\n",
              "             'mockingly': 748,\n",
              "             'welcoming': 749,\n",
              "             'accuse': 750,\n",
              "             'hand': 751,\n",
              "             'away': 752,\n",
              "             'urged': 753,\n",
              "             'admit': 754,\n",
              "             'responsible': 755,\n",
              "             'dragging': 756,\n",
              "             'millions': 757,\n",
              "             'net': 758,\n",
              "             'stamp': 759,\n",
              "             'duty': 760,\n",
              "             'inheritance': 761,\n",
              "             'hide': 762,\n",
              "             'crippling': 763,\n",
              "             'hard-working': 764,\n",
              "             'inevitable': 765,\n",
              "             'wins': 766,\n",
              "             'accused': 767,\n",
              "             'running': 768,\n",
              "             'problems': 769,\n",
              "             'faced': 770,\n",
              "             'answer': 771,\n",
              "             'spend': 772,\n",
              "             'waste': 773,\n",
              "             'liked': 774,\n",
              "             'rattle': 775,\n",
              "             'magical': 776,\n",
              "             'balances': 777,\n",
              "             'conjured': 778,\n",
              "             'thin': 779,\n",
              "             'bid': 780,\n",
              "             'convince': 781,\n",
              "             'nation': 782,\n",
              "             'finances': 783,\n",
              "             'dodgy': 784,\n",
              "             'brought': 785,\n",
              "             'dossier': 786,\n",
              "             'publishing': 787,\n",
              "             'based': 788,\n",
              "             'propose': 789,\n",
              "             'borrow': 790,\n",
              "             'next': 791,\n",
              "             'six': 792,\n",
              "             'less': 793,\n",
              "             'Â£168': 794,\n",
              "             'billion': 795,\n",
              "             ';': 796,\n",
              "             'much': 797,\n",
              "             'prudence': 798,\n",
              "             'forecasts': 799,\n",
              "             'surpluses': 800,\n",
              "             'prime': 801,\n",
              "             'minister': 802,\n",
              "             'weapons': 803,\n",
              "             'mass': 804,\n",
              "             'destruction': 805,\n",
              "             'rebate': 806,\n",
              "             'pensioners': 807,\n",
              "             'Â£300': 808,\n",
              "             'offering': 809,\n",
              "             'nothing': 810,\n",
              "             'put': 811,\n",
              "             'police': 812,\n",
              "             'streets': 813,\n",
              "             'hospitals': 814,\n",
              "             'cleaner': 815,\n",
              "             'give': 816,\n",
              "             'parents': 817,\n",
              "             'teachers': 818,\n",
              "             'discipline': 819,\n",
              "             'skills': 820,\n",
              "             'wanted': 821,\n",
              "             'schools': 822,\n",
              "             'face': 823,\n",
              "             'clear': 824,\n",
              "             'choice': 825,\n",
              "             'higher': 826,\n",
              "             'lower': 827,\n",
              "             'value': 828,\n",
              "             'money': 829,\n",
              "             'conservatives': 830,\n",
              "             'battleground': 831,\n",
              "             'bring': 832,\n",
              "             'concluded': 833,\n",
              "             'loud': 834,\n",
              "             'cheering': 835,\n",
              "             'blu-ray': 836,\n",
              "             'dvd': 837,\n",
              "             'format': 838,\n",
              "             'next-generation': 839,\n",
              "             'rival': 840,\n",
              "             'backers': 841,\n",
              "             'firms': 842,\n",
              "             'sony': 843,\n",
              "             'competing': 844,\n",
              "             'toshiba': 845,\n",
              "             'nec-backed': 846,\n",
              "             'hd-dvd': 847,\n",
              "             'future': 848,\n",
              "             'films': 849,\n",
              "             'thursday': 850,\n",
              "             'giants': 851,\n",
              "             'electronic': 852,\n",
              "             'arts': 853,\n",
              "             'vivendi': 854,\n",
              "             'generation': 855,\n",
              "             'dvds': 856,\n",
              "             'high-definition': 857,\n",
              "             'video': 858,\n",
              "             'sound': 859,\n",
              "             'offers': 860,\n",
              "             'incredible': 861,\n",
              "             '3d-like': 862,\n",
              "             'quality': 863,\n",
              "             'pictures': 864,\n",
              "             'hollywood': 865,\n",
              "             'studios': 866,\n",
              "             'publishers': 867,\n",
              "             'extremely': 868,\n",
              "             'keen': 869,\n",
              "             'exploit': 870,\n",
              "             'separate': 871,\n",
              "             'electronics': 872,\n",
              "             'show': 873,\n",
              "             'las': 874,\n",
              "             'vegas': 875,\n",
              "             'announced': 876,\n",
              "             'technology': 877,\n",
              "             'move': 878,\n",
              "             'standard': 879,\n",
              "             'definition': 880,\n",
              "             'images': 881,\n",
              "             'greater': 882,\n",
              "             'storage': 883,\n",
              "             'richard': 884,\n",
              "             'doherty': 885,\n",
              "             'panasonic': 886,\n",
              "             'laboratories': 887,\n",
              "             'pioneers': 888,\n",
              "             'utilising': 889,\n",
              "             'blue': 890,\n",
              "             'laser-based': 891,\n",
              "             'optical': 892,\n",
              "             'laser': 893,\n",
              "             'disc': 894,\n",
              "             'times': 895,\n",
              "             'today': 896,\n",
              "             'able': 897,\n",
              "             'store': 898,\n",
              "             '50gb': 899,\n",
              "             'high-quality': 900,\n",
              "             'data': 901,\n",
              "             '30gb': 902,\n",
              "             'making': 903,\n",
              "             'sure': 904,\n",
              "             'discs': 905,\n",
              "             'satisfy': 906,\n",
              "             'needs': 907,\n",
              "             'ability': 908,\n",
              "             'onto': 909,\n",
              "             'smaller': 910,\n",
              "             'fit': 911,\n",
              "             'camcorders': 912,\n",
              "             'hopeful': 913,\n",
              "             'emerging': 914,\n",
              "             'war': 915,\n",
              "             'akin': 916,\n",
              "             'betamax': 917,\n",
              "             'vhs': 918,\n",
              "             '1980s': 919,\n",
              "             'resolved': 920,\n",
              "             'come': 921,\n",
              "             'too': 922,\n",
              "             'those': 923,\n",
              "             'huge': 924,\n",
              "             'libraries': 925,\n",
              "             'current': 926,\n",
              "             'big': 927,\n",
              "             'boost': 928,\n",
              "             'battle': 929,\n",
              "             'supremacy': 930,\n",
              "             '$': 931,\n",
              "             'industry': 932,\n",
              "             'crucial': 933,\n",
              "             'film': 934,\n",
              "             'terms': 935,\n",
              "             'technical': 936,\n",
              "             'requirement': 937,\n",
              "             'development': 938,\n",
              "             'demands': 939,\n",
              "             'advanced': 940,\n",
              "             'optical-disc': 941,\n",
              "             'technologies': 942,\n",
              "             'heilmann': 943,\n",
              "             'officer': 944,\n",
              "             'universal': 945,\n",
              "             'capacity': 946,\n",
              "             'performance': 947,\n",
              "             'high-speed': 948,\n",
              "             'internet': 949,\n",
              "             'connectivity': 950,\n",
              "             'ea': 951,\n",
              "             'developer': 952,\n",
              "             'publisher': 953,\n",
              "             'delivery': 954,\n",
              "             'vital': 955,\n",
              "             'functionality': 956,\n",
              "             'interactivity': 957,\n",
              "             'kinds': 958,\n",
              "             'projects': 959,\n",
              "             'planning': 960,\n",
              "             'recently': 961,\n",
              "             'using': 962,\n",
              "             'playstations': 963,\n",
              "             'ravenous': 964,\n",
              "             'graphics': 965,\n",
              "             'titles': 966,\n",
              "             'especially': 967,\n",
              "             'pcs': 968,\n",
              "             'always': 969,\n",
              "             'focused': 970,\n",
              "             'deliver': 971,\n",
              "             'textures': 972,\n",
              "             'deeper': 973,\n",
              "             'levels': 974,\n",
              "             'delivering': 975,\n",
              "             'higher-resolution': 976,\n",
              "             'playback': 977,\n",
              "             'moving': 978,\n",
              "             'forward': 979,\n",
              "             'increased': 980,\n",
              "             'immersion': 981,\n",
              "             'really': 982,\n",
              "             'creating': 983,\n",
              "             'involves': 984,\n",
              "             'complicated': 985,\n",
              "             '3d': 986,\n",
              "             'models': 987,\n",
              "             'increasing': 988,\n",
              "             'resolution': 989,\n",
              "             'frame': 990,\n",
              "             'rate': 991,\n",
              "             'getting': 992,\n",
              "             'immersive': 993,\n",
              "             'experience': 994,\n",
              "             'fitting': 995,\n",
              "             'means': 996,\n",
              "             'compressing': 997,\n",
              "             'lost': 998,\n",
              "             'photo-real': 999,\n",
              "             'capability': 1000,\n",
              "             ...})"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import collections\n",
        "from nltk import word_tokenize\n",
        "## Set the size of the default value that will be assigned to each token to \n",
        "## the current size of the vocabulary\n",
        "token2int = collections.defaultdict(lambda: len(token2int)+1) \n",
        "\n",
        "# Create the dictionary from a list of tokens\n",
        "tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "int_texts = [[token2int[token.lower()] for token in text] for text in tokenized_texts]\n",
        "\n",
        "token2int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHT_bKtua7lJ"
      },
      "source": [
        "#### Exercise 2 - Define the reverse int2token mapping and check the token2int and the int2token mappings on an example\n",
        "\n",
        "* Check that the words have been correctly converted to integer by applying the reverse integer to token mapping\n",
        "* Print out the vocabulary size \n",
        "* Print out the maximum text length \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrpYmNJA3kGi",
        "outputId": "09339896-bfa1-442b-fc4d-ad8ce3b02358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1604\n",
            "horse\n",
            "The Vocabulary size is: 34594\n",
            "The maximum text length: 4969\n"
          ]
        }
      ],
      "source": [
        "int2token = dict()\n",
        "for key, value in token2int.items():\n",
        "    int2token[value] = key\n",
        "\n",
        "## Exemple to check if it performs correctly\n",
        "print(token2int['horse'])\n",
        "print(int2token[1604])\n",
        "\n",
        "## Vocabulary Size:\n",
        "print(\"The Vocabulary size is:\",len(token2int))\n",
        "\n",
        "## Maximum text length\n",
        "text_lengths = [len(text) for text in tokenized_texts]\n",
        "print(\"The maximum text length:\", max(text_lengths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50OEV582CCgC"
      },
      "source": [
        "#### Exercise 3 - Compute some statistics to determine which sentences to keep\n",
        "\n",
        "The maximum sentence size is very large (4432 tokens). Presumably most reports are not that long. Use pandas describe() method and matplotlib to get a better idea of the data distribution in terms of report length.\n",
        "\n",
        "* Compute the list of report lengths (list of nb of tokens for each report in the BBC news corpus)\n",
        "* Compute the box plots for report lengths\n",
        "* Use pandas describe() method to get the descripte statistics (min, max, means, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "lYeCsmeCCCgD",
        "outputId": "02edef79-8f4b-4763-a7b5-858659dc1f1b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c4f40040-aabe-4289-a899-c43a95d1b0e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2225.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>438.204045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>269.705388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>98.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>280.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>380.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>538.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4969.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4f40040-aabe-4289-a899-c43a95d1b0e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4f40040-aabe-4289-a899-c43a95d1b0e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4f40040-aabe-4289-a899-c43a95d1b0e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 0\n",
              "count  2225.000000\n",
              "mean    438.204045\n",
              "std     269.705388\n",
              "min      98.000000\n",
              "25%     280.000000\n",
              "50%     380.000000\n",
              "75%     538.000000\n",
              "max    4969.000000"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df1 = pd.DataFrame(text_lengths)\n",
        "\n",
        "## BOXPLOT:\n",
        "df1.boxplot()\n",
        "plt.title(\"Boxplot of nb of tokens\")\n",
        "plt.show()\n",
        "\n",
        "## Description of the data\n",
        "df1.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxs8QE1Ha7ld",
        "tags": []
      },
      "source": [
        "### 2  Classifying with a Recurrent Neural Network\n",
        "\n",
        "We import some pytorch submodules (PROVIDED)\n",
        "\n",
        "* torch: functions to create tensors and operations on tensors \n",
        "* torch.nn: to specify neural networks\n",
        "* torch.nn.functional for when we want to define a custom layer for example with a convolution operation layer.\n",
        "* torch.optim: optimizers for training \n",
        "\n",
        "We also need to specify some constants which ensure that the code can run on CPU. The `max_len` constant is important as text whose size exceeds (in number of tokens) that limit will be discarded. We set it to 555 (see next). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOlSjYtuCCgG",
        "outputId": "ed60bcd3-0111-405c-cb6e-421eb52fb9e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install torch==2.0.0 torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "SU-ibO_-a7ll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C9UoOQka7l7"
      },
      "source": [
        "#### Exercise 4 - Creating tensors\n",
        "\n",
        "To help speed up computation, all data must be converted to tensors. \n",
        "\n",
        "* Set the maximum size of text to the 3rd quartile (555)\n",
        "* Create a tensor of size  (number of texts, maximum size of a text) for X. Call this tensor X. Longer texts will be cut down to maximal size and shorter texts will be padded with `<eos>`. Use torch.zeros method and make sure to specify the components will be of type integer (long attribute in torch)\n",
        "* Populate this matrix with the integer version of the BBC news report (cf. Exercise 1). Use the torch.LongTensor method. When populating the matrix cut down sentence whose length is above the max length to max length\n",
        "* Create another tensor of size (numbers of text) and populate it with the list of labels. Call this tensor Y.\n",
        "* Print out the shape of X and Y . X should be of size (2225, 555) and Y of size (2225,) \n",
        "\n",
        "N.B. In practice, it is not necessary to have all the data in a single tensor. In fact, this is inefficient if the texts have varied length. The only constraint is that for a given batch, all sequences have the same size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxgadeyPCCgI",
        "outputId": "ccb1942a-a6a6-42f5-9814-f70524e82a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2225, 555])\n",
            "torch.Size([2225])\n"
          ]
        }
      ],
      "source": [
        "## Specifying the maximum length, which is equivalent to the 3rd quartile.\n",
        "## We should pay attention that the value of the third quartile is 538, \n",
        "## while in the given it was specified as 555\n",
        "\n",
        "max_len = 555\n",
        "\n",
        "## Tensor is the equivalent of pytorch of a list/table/matrix with multiple\n",
        "## dimensions\n",
        "## We create the table of dimentsion  and we fill it with 0\n",
        "X = torch.zeros(len(texts), max_len, dtype=torch.long)\n",
        "## print(X.size()) # To check the length\n",
        "## print(X) # et To check the content\n",
        "\n",
        "## We fill the tensor line with the text  line converted to natural numbers\n",
        "for i, int_text in enumerate(int_texts): ## For each text\n",
        "    if len(int_text) < max_len: ## In case the text is too short.\n",
        "        int_text = int_text + [len(token2int)] * (max_len - len(int_text))\n",
        "        ## We obtain the best length\n",
        "\n",
        "    X[i] = torch.LongTensor(int_text[:max_len]) ## fill the corresponding rows.\n",
        "\n",
        "## Create another tensor of size (numbers of text) and populate it with\n",
        "##  the list of labels. Call this tensor Y.\n",
        "Y = torch.LongTensor(labels)\n",
        "\n",
        "## Print out the shape of X and Y . X should be of size (2225, 555) \n",
        "## and Y of size (2225,) \n",
        "print(X.size())\n",
        "print(Y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eakhMa5Sa7mL"
      },
      "source": [
        "#### Exercise 5 - Create train and test data\n",
        "\n",
        "* Split X into two parts, one called X_train which consists of the first 1112 items and the other called X_valid which includes the rest of the data\n",
        "* Do the same for Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "AgbuVUlWCCgN"
      },
      "outputs": [],
      "source": [
        "X_train = X[:1112]\n",
        "Y_train = Y[:1112]\n",
        "\n",
        "X_valid = X[1112:]\n",
        "Y_valid = Y[1112:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZkXMuF0a7mj"
      },
      "source": [
        "#### Create batches with DataLoader (PROVIDED)\n",
        "\n",
        "pytorch provides a batch generator which shuffles the data. We apply it to train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "veYFlstja7mm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
        "# Note that input_features and labels must match on the length of the first dimension\n",
        "train_set = TensorDataset(X_train, Y_train)\n",
        "valid_set = TensorDataset(X_valid, Y_valid)\n",
        "\n",
        "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwh1VKE1a7n1"
      },
      "source": [
        "#### Exercise 6 - Define your neural network (PARTIALLY PROVIDED)\n",
        "\n",
        "We define a network by specifying a subclass of the appropriate pytorch modules. For instance here, as we want to create an RNN we create a subclass of pytorch RNN module. The specification of the network falls into 2 parts. \n",
        "\n",
        "In the init part, the layers of the network are defined and  their type and shape are specified.  \n",
        "\n",
        "In the forward part, we connect the layers and specify input and output for each layer. \n",
        " \n",
        "* The hidden state returned by the GRU layer is of size (num_layers * num_directions, batch_size, hidden_size) \n",
        "* The input to the decision layer should be of size (batch_size, hidden_size).   \n",
        "\n",
        "Hence the first 2 dimensions of hidden must be transposed and the tensor redimensioned to (batch_size, hidden_size).\n",
        "\n",
        "* drop : (num_layers * num_directions, batch_size, hidden_size)\n",
        "* drop.transpose(0,1): (batch_size, num_layers * num_directions,  hidden_size)\n",
        "* x.size(0) = batch_size\n",
        "* drop.transpose(0,1).contiguous.view(x.size(0), -1): (batchsize, hiddensize)\n",
        "\n",
        "**TODO:** fill in the missing variables (marked by ???)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyV6gWoOa7n7",
        "outputId": "8fd9bb22-e965-4eaa-a8c4-373c45f1db47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(34595, 300)\n",
              "  (rnn): GRU(300, 256, batch_first=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (decision): Linear(in_features=256, out_features=686, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Here we define the network layers\n",
        "        \n",
        "        # An embedding layer projecting vectors of size vocab_size into embeddings of size embed_size\n",
        "        # Assigns to each word in the vocabulary an embedding of size embed_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        \n",
        "        # A recurrent (GRU) layer to process each input token (represented by its embedding)\n",
        "        # The GRU network takes as input the embedding (of size embed_size) of the current word \n",
        "        # and the previous hidden state (of size hidden_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True) \n",
        "        \n",
        "        # Drop out layer for regularisation\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # Fully connected layer mapping \n",
        "        # the last layer maps a hidden state to a vector of size the number of classes\n",
        "        self.decision = nn.Linear(hidden_size, num_classes)\n",
        "         \n",
        "    def forward(self, x):\n",
        "        # Here we say how the layers are connected \n",
        "       \n",
        "        #  for each token in the input, retrieve the corresponding embeddings \n",
        "        \n",
        "        # x = [batch size, input size]\n",
        "        embed = self.embed(x)\n",
        "        # embed = [batch size, sent len, emb dim]       \n",
        "       \n",
        "        # Run the RNN on the input embeddings\n",
        "        # output is the sequence of hidden states produced by the RNN\n",
        "        # hidden is the last hidden state produced\n",
        "        output, hidden = self.rnn(embed)\n",
        "        \n",
        "        # output = [sent len, batch size, hidden size]\n",
        "        # hidden = [num_layers * num_directions, batch size, hidden_size ]\n",
        "        \n",
        "        # Apply dropout (for regularisation)\n",
        "        drop = self.dropout(hidden)\n",
        "        \n",
        "        # drop = [num_layers * num_directions, batch size, hidden_size]   \n",
        "        \n",
        "        # Apply the fully connected layer to the output of the dropout\n",
        "        # Expected input size: [batch_size, input_size]\n",
        "        # We transpose [num_layers * num_directions, batch size, hidden_size ]\n",
        "        # to: [batch size, num_layers * num_directions, hidden_size ]\n",
        "        # And apply view to create an input of the form [batch size, input_size ]\n",
        "        # (x.size(0) = batch size)\n",
        "        \n",
        "        # effectively the same as with drop.squeeze(0)\n",
        "        return self.decision(drop.view(x.size(0), -1))\n",
        "    \n",
        "rnn_model = RNN(vocab_size = len(token2int)+1, embed_size = 300, \n",
        "                hidden_size = 256, num_classes=len(df1[0].unique()))\n",
        "device = torch.device('cpu')\n",
        "rnn_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3276ujaa7nF"
      },
      "source": [
        "#### Exercise 7  - Evaluating (PARTIALLY PROVIDED)\n",
        "\n",
        "We define an evaluation function (called \"perf\") which computes the average loss on the test/validation dataset and the proportion of correct cases. \n",
        "\n",
        "* We use pytorch nn.CrossEntropyLoss() as loss function.\n",
        "* For each batch returned by the loader, we calculate the scores produced by the model for each class, the loss, the predictions and the loss.\n",
        "* To block dropout (which should only be used at training time), we use the eval() method. \n",
        "* \"with torch.no_grad()\" temporarily set all the requires_grad flag to false. In practice, this enforces that gradients are not computed (and therefore the weights of the model remain unchanged). \n",
        "\n",
        "**TODO:** Modify the function so that it outputs the proportion of correct predictions in addition to the loss. Replace the '??' in the function with the correct variables and/or operations on these variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "WW8p4Pnga7nH"
      },
      "outputs": [],
      "source": [
        "def perf(model, loader):\n",
        "    ## Define the loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ## No drop out\n",
        "    model.eval()\n",
        "    total_loss, correct, num = 0, 0, 0\n",
        "    correct_predictions, total_predictions = 0, 0\n",
        "    for x, y in loader:\n",
        "      ## No gradient computation, weights remain unchanged\n",
        "      with torch.no_grad():\n",
        "        ## Compute the scofed for the instances in the input batch.\n",
        "        y_scores = model(x)\n",
        "        ## compute the loss\n",
        "        predictions = torch.argmax(y_scores, dim=1)\n",
        "        ## compute the predictions\n",
        "        correct_predictions += sum(predictions == y).item() \n",
        "\n",
        "        total_predictions += len(y) \n",
        "        loss = criterion(y_scores, y)\n",
        "        y_pred = torch.max(y_scores, 1)[1]\n",
        "        ## Update the batch loss.\n",
        "        total_loss += loss.item()\n",
        "        num += len(y)\n",
        "\n",
        "    return total_loss / num, correct_predictions / total_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFjr1JIqCCgS",
        "outputId": "dedf2e29-6598-4f2f-9b5d-96cc3858e0a6",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.20527846172706352, 0.0)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perf(model = rnn_model, loader = valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1gS2QLACCgT"
      },
      "source": [
        "#### Exercise 8  - Training Loop\n",
        "\n",
        "Define a function fit(model, epochs) which you will use to train your model\n",
        "\n",
        "* use the CrossEntropyLoss and the Adam optimizer\n",
        "* iterates over the epochs and for each epoch:\n",
        "   - Set the module in training mode (use the train() method). Dropout will be enabled and gradients will be computed. \n",
        "   - Initialise the total loss to 0\n",
        "   - Iterate over each batches returned by train_loader    \n",
        "     For each batch:   \n",
        "        - set the gradients to null (optimizer.zero_grad())\n",
        "        - predicts the batch scores\n",
        "        - calculate the loss\n",
        "        - back propagate\n",
        "        - optimize (adjust the weights)\n",
        "        - update the total loss\n",
        "   \n",
        "**Hint:** Some of these steps are defined in the preceding exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "uRu1s_QECCgT"
      },
      "outputs": [],
      "source": [
        "def fit(model, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_samples = 0\n",
        "        \n",
        "        for x_data, y_data in train_loader:\n",
        "            x_data = x_data.to(device)\n",
        "            y_data = y_data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x_data)\n",
        "            num_samples += len(y_pred)\n",
        "            loss = criterion(y_pred, y_data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        valid_loss, valid_acc = perf(model, valid_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} | Train loss: {total_loss/num_samples:.4f} | Valid loss: {valid_loss:.4f} | Acc: {valid_acc:.4%}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nlDoZhxa7oo"
      },
      "source": [
        "#### Exercise 9 - Training\n",
        "\n",
        "Use the fit function you just defined to train your model. \n",
        "* The loss function on the train and validation set should decrease\n",
        "* The accuracy on the validation set should improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHxm9ySWCCgU",
        "outputId": "b818eaf2-c9d7-4122-b639-2d2705601e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train loss: 0.0508 | Valid loss: 0.0486 | Acc: 28.0323%\n"
          ]
        }
      ],
      "source": [
        "## Running this code will take a lot of time\n",
        "fit(rnn_model, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6w-3uokCCgt"
      },
      "source": [
        "### 3 Classifying with a Convolutional Neural Network (OPTIONAL)\n",
        "\n",
        "Here below is a CNN model definition. Run it on the data and compare the speed and results with that of the RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-MaqpBcCCgu",
        "outputId": "dcf36cef-f389-4f61-a3d8-8738cc0031f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embed): Embedding(34594, 64)\n",
              "  (conv): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (decision): Linear(in_features=128, out_features=686, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.conv = nn.Conv1d(embed_size, hidden_size, kernel_size=2)\n",
        "        self.dropout = nn.Dropout(.1)\n",
        "        self.decision = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embed(x)\n",
        "        conv = F.relu(self.conv(embed.transpose(1,2)))\n",
        "        pool = F.max_pool1d(conv, conv.size(2))\n",
        "        drop = self.dropout(pool)\n",
        "        return self.decision(drop.view(x.size(0), -1))\n",
        "\n",
        "cnn_model = CNN(vocab_size = len(token2int), embed_size = 64, \n",
        "                hidden_size = 128, num_classes=len(df1[0].unique()))\n",
        "cnn_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjDJTq6HCCgv",
        "outputId": "f23e9d31-5022-4ead-96b3-47d811d12f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25 | Train loss: 0.0500 | Valid loss: 0.0464 | Acc: 40.1617%\n",
            "Epoch 2/25 | Train loss: 0.0410 | Valid loss: 0.0392 | Acc: 73.2255%\n",
            "Epoch 3/25 | Train loss: 0.0337 | Valid loss: 0.0314 | Acc: 79.4250%\n",
            "Epoch 4/25 | Train loss: 0.0250 | Valid loss: 0.0247 | Acc: 82.0305%\n",
            "Epoch 5/25 | Train loss: 0.0185 | Valid loss: 0.0196 | Acc: 86.8823%\n",
            "Epoch 6/25 | Train loss: 0.0145 | Valid loss: 0.0166 | Acc: 84.8158%\n",
            "Epoch 7/25 | Train loss: 0.0114 | Valid loss: 0.0146 | Acc: 86.8823%\n",
            "Epoch 8/25 | Train loss: 0.0096 | Valid loss: 0.0122 | Acc: 89.8473%\n",
            "Epoch 9/25 | Train loss: 0.0077 | Valid loss: 0.0115 | Acc: 90.1168%\n",
            "Epoch 10/25 | Train loss: 0.0060 | Valid loss: 0.0100 | Acc: 92.0934%\n",
            "Epoch 11/25 | Train loss: 0.0050 | Valid loss: 0.0095 | Acc: 91.4645%\n",
            "Epoch 12/25 | Train loss: 0.0043 | Valid loss: 0.0091 | Acc: 91.8239%\n",
            "Epoch 13/25 | Train loss: 0.0037 | Valid loss: 0.0082 | Acc: 92.8122%\n",
            "Epoch 14/25 | Train loss: 0.0030 | Valid loss: 0.0077 | Acc: 93.1716%\n",
            "Epoch 15/25 | Train loss: 0.0030 | Valid loss: 0.0074 | Acc: 93.3513%\n",
            "Epoch 16/25 | Train loss: 0.0027 | Valid loss: 0.0072 | Acc: 93.1716%\n",
            "Epoch 17/25 | Train loss: 0.0023 | Valid loss: 0.0069 | Acc: 93.4412%\n",
            "Epoch 18/25 | Train loss: 0.0020 | Valid loss: 0.0066 | Acc: 94.5193%\n",
            "Epoch 19/25 | Train loss: 0.0017 | Valid loss: 0.0063 | Acc: 94.4295%\n",
            "Epoch 20/25 | Train loss: 0.0015 | Valid loss: 0.0063 | Acc: 93.9802%\n",
            "Epoch 21/25 | Train loss: 0.0015 | Valid loss: 0.0059 | Acc: 94.6092%\n",
            "Epoch 22/25 | Train loss: 0.0011 | Valid loss: 0.0058 | Acc: 94.6092%\n",
            "Epoch 23/25 | Train loss: 0.0012 | Valid loss: 0.0059 | Acc: 94.6092%\n",
            "Epoch 24/25 | Train loss: 0.0013 | Valid loss: 0.0058 | Acc: 94.9686%\n",
            "Epoch 25/25 | Train loss: 0.0009 | Valid loss: 0.0056 | Acc: 94.4295%\n"
          ]
        }
      ],
      "source": [
        "cnn_model = CNN(vocab_size=len(token2int)+1, embed_size=64, hidden_size=128, num_classes=len(df.labels.unique()))\n",
        "cnn_model.to(device)\n",
        "fit(cnn_model, 25)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
