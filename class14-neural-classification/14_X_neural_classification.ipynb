{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7N-x97sa7h4"
   },
   "source": [
    "# Exercise \"Lecture 14: Neural Classification\"\n",
    "\n",
    "\n",
    "In this set of exercises, we will use a Recurrent Neural Network to classify BBC news articles into 5 topics. The dataset consists of 2225 documents and 5 categories: business, entertainment, politics, sport, and technology. \n",
    "\n",
    "\n",
    "The exercises cover the following points:\n",
    "\n",
    "* Converting the text in the corpus to vectors of integers (each integer represents a word in the corpus vocabulary)\n",
    "* Computing some descriptive statistics to identify a sentence length cutoff (sentences with longer lengths will not be considered for training)\n",
    "* Specifying, training and testing a recurrent neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing (PROVIDED)\n",
    "\n",
    "We first prepocessed the data to extract X (the input) and Y (the input labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_report</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tate &amp; Lyle boss bags top award\\n\\nTate &amp; Lyle...</td>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Halo 2 sells five million copies\\n\\nMicrosoft ...</td>\n",
       "      <td>4</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSPs hear renewed climate warning\\n\\nClimate c...</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pavey focuses on indoor success\\n\\nJo Pavey wi...</td>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tories reject rethink on axed MP\\n\\nSacked MP ...</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         news_report  labels labels_text\n",
       "0  Tate & Lyle boss bags top award\\n\\nTate & Lyle...       0    business\n",
       "1  Halo 2 sells five million copies\\n\\nMicrosoft ...       4        tech\n",
       "2  MSPs hear renewed climate warning\\n\\nClimate c...       2    politics\n",
       "3  Pavey focuses on indoor success\\n\\nJo Pavey wi...       3       sport\n",
       "4  Tories reject rethink on axed MP\\n\\nSacked MP ...       2    politics"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "\n",
    "# for reproducibility\n",
    "random_state = 0 \n",
    "\n",
    "DATA_DIR = \"bbc/\"\n",
    "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\", random_state=random_state)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'news_report': data['data'], 'labels': data['target']})\n",
    "# add text labels \n",
    "df['labels_text'] = df.labels.apply(lambda x: data['target_names'][x])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xchG9cxna7jE"
   },
   "source": [
    "Extracting X (texts) and Y (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjVnCCXNa7jJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain coach faces racism inquiry\n",
      "\n",
      "Spain's Football Federation has initiated disciplinary action against national coach Luis Aragones over racist comments about Thierry Henry.\n",
      "\n",
      "If found guilty Aragones could lose his job or face a fine of about Â£22,000. The federation had initially declined to take action against Aragones after comments he made during a national team training session in October. But its president Angel Maria Villar changed his mind after a request by Spain's anti-violence commission. Aragones insisted the comments, made to Henry's Arsenal club-mate Jose Antonio Reyes, were meant to motivate the player, and were not intended to be offensive.\n",
      "\n",
      "\"I never intended to offend anyone, and for that reason I have a very easy conscience,\" he said at the time. \"I'm obliged to motivate my players to get the best results. \"As part of that job, I use colloquial language, with which we can all understand each other within the framework of the football world. \" England's players made a point of wearing anti-racism t-shirts when training before their friendly against Spain in Madrid last month.\n",
      "\n",
      "But the storm increased following racist chanting by Spanish fans at England's black players during the game, which Spain won 1-0. Spain's minister of sport Jaime Lissavetzky was quick to give his backing to the Federation's decision. \"Everyone who has a public function has to consider their declarations, and make sure they do not give a negative image,\" he said. \"We are going to have zero tolerance in questions of racism.\"\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "texts = df[\"news_report\"]\n",
    "labels = df[\"labels\"]\n",
    "\n",
    "n=100\n",
    "print(texts[n])\n",
    "print(labels[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Converting the texts in the corpus to vectors of integers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYbQI1mka7kl"
   },
   "source": [
    "#### Exercise 1 -  Convert the corpus to a list of lists of integers\n",
    "\n",
    "* Define a dictionary tokens2int which maps each distinct token in the corpus to a distinct integer \n",
    "(the size of this dictionary is the size of the corpus vocabulary i.e., the number of distinct tokens in the corpus, cf. Python CS)\n",
    "* Use this dictionary to map each news report to a vector.\n",
    "\n",
    "\n",
    "**Example**\n",
    "* Input Texts: [\"The woman put the book on the table\", \"The woman reads\"]\n",
    "* Created vocabulary: {the, woman, put, book, on, table, reads}\n",
    "* Output Texts: [ [1,2,3,1,4,5,1,6], [1,2,7]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34731\n"
     ]
    }
   ],
   "source": [
    "global_text = list(texts)\n",
    "\n",
    "# get the vocabulary\n",
    "voc = set()\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for text in global_text:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        voc.add(token.text)\n",
    "\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the vocabulary into a list, so that we fix an order\n",
    "voc_list = list(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping dictionnary from token to index\n",
    "tokens2int = {w: i for i, w in enumerate(voc_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32537"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2int[\"am\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this mapping to convert texts into lists of integers\n",
    "texts_as_ints = [[tokens2int[w.text] for w in nlp(text)] for text in global_text]\n",
    "df[\"texts_as_ints\"] = texts_as_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_report</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_text</th>\n",
       "      <th>texts_as_ints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tate &amp; Lyle boss bags top award\\n\\nTate &amp; Lyle...</td>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>[27237, 28574, 14131, 10250, 20486, 17805, 317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Halo 2 sells five million copies\\n\\nMicrosoft ...</td>\n",
       "      <td>4</td>\n",
       "      <td>tech</td>\n",
       "      <td>[27597, 17213, 10454, 6732, 26691, 31975, 4962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSPs hear renewed climate warning\\n\\nClimate c...</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "      <td>[27016, 26658, 1644, 21692, 5309, 4962, 23464,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pavey focuses on indoor success\\n\\nJo Pavey wi...</td>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>[7490, 31371, 8185, 14519, 25407, 4962, 24702,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tories reject rethink on axed MP\\n\\nSacked MP ...</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "      <td>[13999, 32269, 2590, 8185, 11619, 14723, 4962,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         news_report  labels labels_text  \\\n",
       "0  Tate & Lyle boss bags top award\\n\\nTate & Lyle...       0    business   \n",
       "1  Halo 2 sells five million copies\\n\\nMicrosoft ...       4        tech   \n",
       "2  MSPs hear renewed climate warning\\n\\nClimate c...       2    politics   \n",
       "3  Pavey focuses on indoor success\\n\\nJo Pavey wi...       3       sport   \n",
       "4  Tories reject rethink on axed MP\\n\\nSacked MP ...       2    politics   \n",
       "\n",
       "                                       texts_as_ints  \n",
       "0  [27237, 28574, 14131, 10250, 20486, 17805, 317...  \n",
       "1  [27597, 17213, 10454, 6732, 26691, 31975, 4962...  \n",
       "2  [27016, 26658, 1644, 21692, 5309, 4962, 23464,...  \n",
       "3  [7490, 31371, 8185, 14519, 25407, 4962, 24702,...  \n",
       "4  [13999, 32269, 2590, 8185, 11619, 14723, 4962,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHT_bKtua7lJ"
   },
   "source": [
    "#### Exercise 2 - Define the reverse int2token mapping and check the token2int and the int2token mappings on an example\n",
    "\n",
    "* Check that the words have been correctly converted to integer by applying the reverse integer to token mapping\n",
    "* Print out the vocabulary size \n",
    "* Print out the maximum text length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reverse mapping\n",
    "int2tokens = {i: w for i, w in enumerate(voc_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Back-conversion:\n",
      " Tate & Lyle boss bags top award \n",
      "\n",
      " Tate & Lyle 's chief executive has been named European Businessman of the Year by a leading business magazine . \n",
      "\n",
      " Iain Ferguson was awarded the title by US publication Forbes for returning one of the UK 's \" venerable \" manufacturers to the country 's top 100 companies . The sugar group had been absent from the FTSE 100 for seven years until Mr Ferguson helped it return to growth . Tate 's shares have leapt 55 % this year , boosted by firming sugar prices and sales of its artificial sweeteners . \n",
      "\n",
      " \" After years of a sagging stock price and a seven - year hiatus from the FTSE 100 , one of Britain 's venerable manufacturers has returned to the vaunted index , \" Forbes said . Mr Ferguson took the helm at the company in 2003 , after spending most of his career at consumer goods giant Unilever . Tate & Lyle , which was an original member of the historic FT-30 index in 1935 , operates more than 41 factories and 20 more additional production facilities in 28 countries . Previous winners of the Forbes award include Royal Bank of Scotland chief executive Fred Goodwin and former Vodafone boss Chris Gent . \n",
      "\n",
      "\n",
      "** Original:\n",
      " Tate & Lyle boss bags top award\n",
      "\n",
      "Tate & Lyle's chief executive has been named European Businessman of the Year by a leading business magazine.\n",
      "\n",
      "Iain Ferguson was awarded the title by US publication Forbes for returning one of the UK's \"venerable\" manufacturers to the country's top 100 companies. The sugar group had been absent from the FTSE 100 for seven years until Mr Ferguson helped it return to growth. Tate's shares have leapt 55% this year, boosted by firming sugar prices and sales of its artificial sweeteners.\n",
      "\n",
      "\"After years of a sagging stock price and a seven-year hiatus from the FTSE 100, one of Britain's venerable manufacturers has returned to the vaunted index,\" Forbes said. Mr Ferguson took the helm at the company in 2003, after spending most of his career at consumer goods giant Unilever. Tate & Lyle, which was an original member of the historic FT-30 index in 1935, operates more than 41 factories and 20 more additional production facilities in 28 countries. Previous winners of the Forbes award include Royal Bank of Scotland chief executive Fred Goodwin and former Vodafone boss Chris Gent.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that the reverse mapping is correct\n",
    "int_sentence = df.iloc[0].texts_as_ints\n",
    "print(\"** Back-conversion:\\n\", \" \".join([int2tokens[i] for i in int_sentence]))\n",
    "print(\"\\n** Original:\\n\", df.iloc[0].news_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  34731\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: \",len(tokens2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum text length:  5062\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum text length: \", max([len(text) for text in texts_as_ints]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 - Compute some statistics to determine which sentences to keep\n",
    "\n",
    "The maximum sentence size is very large (4432 tokens). Presumably most reports are not that long. Use pandas describe() method and matplotlib to get a better idea of the data distribution in terms of report length.\n",
    "\n",
    "* Compute the list of report lengths (list of nb of tokens for each report in the BBC news corpus)\n",
    "* Compute the box plots for report lengths\n",
    "* Use pandas describe() method to get the descripte statistics (min, max, means, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2225.000000\n",
       "mean      454.232809\n",
       "std       277.552961\n",
       "min       103.000000\n",
       "25%       292.000000\n",
       "50%       394.000000\n",
       "75%       555.000000\n",
       "max      5062.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"length\"] = df[\"texts_as_ints\"].apply(lambda x: len(x))\n",
    "df[\"length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGzCAYAAADNKAZOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAArYUlEQVR4nO3deXxU9b3/8fdk3xMgCzuhgKwKyi4gKJSIiCzlgkI1sSqCrA8BxXorIJVQgV4pKBXtBetFqJatROQKGEBlE2QJSxEkLMoSUAJBEkKS7+8PfpnLZIEEkkzy5fV8POYhc853vudzPnMwb86cM3EYY4wAAAAs4+HuAgAAAEoDIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhB7iDLViwQA6HQ0ePHnV3KTd09OhRORwOzZgxo0TnLWj/u3Tpoi5dupTodgrjcDg0adIk5/NJkybJ4XDo3LlzZbL96OhoxcXFlcm2AHcg5AClyOFwFOmxfv36EtneyZMnNWnSJO3atatE5itrq1atcvmhX1Fs2rRJkyZNUmpqqrtLyac81waUNi93FwDY7MMPP3R5/ve//11r1qzJt7xx48Ylsr2TJ09q8uTJio6OVosWLUpkzrK0atUqvf32224NOp9//nmxX7Np0yZNnjxZcXFxCgsLK/Lr0tPT5eVVuv8bvlFtBw8elIcH/9aFvQg5QCn67W9/6/J8y5YtWrNmTb7lKD98fHxKdf6cnBxlZmbKz89Pfn5+pbqtm/H19XXr9oHSRoQH3CwnJ0dvvfWWmjZtKj8/P0VFRen555/X+fPnnWMmTpwoDw8PrVu3zuW1Q4YMkY+Pj3bv3q3169erdevWkqSnn37a+VHYggULil3TZ599pk6dOikwMFDBwcHq2bOn9u3b5zImLi5OQUFB+vHHH9WnTx8FBQUpIiJC48aNU3Z2tsvYn376SU8++aRCQkIUFham2NhY7d6926W+uLg4vf3225JcP+bLa968eapXr558fX3VunVrffPNN0Xap3379umhhx6Sv7+/atasqT/+8Y/KycnJN66ga3Jmz56tpk2bKiAgQJUqVVKrVq300UcfSbp2Hc348eMlSXXr1nXWnXudj8Ph0IgRI7Rw4UI1bdpUvr6+Wr16tXNdQWetzp07pwEDBigkJERVqlTR6NGjlZGR4Vyfe41SQe/t9XPerLaCrsk5cuSI/uM//kOVK1dWQECA2rVrp08//dRlzPr16+VwOPTxxx/rjTfeUM2aNeXn56euXbvq8OHD+WoC3IUzOYCbPf/881qwYIGefvppjRo1SsnJyZozZ4527typr7/+Wt7e3vrP//xPrVy5Us8884ySkpIUHBys//3f/9V7772nKVOmqHnz5jpz5oxef/11vfbaaxoyZIg6deokSbr//vuLVc+HH36o2NhYxcTE6E9/+pMuX76suXPnqmPHjtq5c6eio6OdY7OzsxUTE6O2bdtqxowZWrt2rWbOnKl69epp2LBhkq6FuF69emnbtm0aNmyYGjVqpBUrVig2NjZfH06ePFngx3m5PvroI6Wlpen555+Xw+HQm2++qX79+unIkSPy9vYudJ9Onz6tBx98UFlZWZowYYICAwM1b948+fv737Qf7733nkaNGqX+/fs7w8aePXu0detWDRo0SP369dN3332nRYsW6b/+678UHh4uSYqIiHDO8cUXX+jjjz/WiBEjFB4e7tLDggwYMEDR0dGKj4/Xli1b9Je//EXnz5/X3//+95vWe72i1Ha9M2fO6P7779fly5c1atQoValSRR988IEee+wx/fOf/1Tfvn1dxk+bNk0eHh4aN26cLly4oDfffFODBw/W1q1bi1UnUGoMgDIzfPhwc/1fuy+//NJIMgsXLnQZt3r16nzLk5KSjI+Pj3n22WfN+fPnTY0aNUyrVq3M1atXnWO++eYbI8nMnz+/SPXMnz/fSDLJycnGGGPS0tJMWFiYee6551zGnT592oSGhrosj42NNZLM66+/7jL23nvvNS1btnQ+X7JkiZFk3nrrLeey7Oxs89BDD+WrNW9/ciUnJxtJpkqVKubnn392Ll+xYoWRZFauXHnD/RwzZoyRZLZu3epclpKSYkJDQ1323xhjOnfubDp37ux83rt3b9O0adMbzj99+vR88+SSZDw8PMy+ffsKXDdx4kTn84kTJxpJ5rHHHnMZ98ILLxhJZvfu3caY/+tHQe9z3jlvVFudOnVMbGys83lun7788kvnsrS0NFO3bl0THR1tsrOzjTHGJCYmGkmmcePG5sqVK86xs2bNMpJMUlJSvm0B7sDHVYAbffLJJwoNDdWvf/1rnTt3zvlo2bKlgoKClJiY6BzbrFkzTZ48We+//75iYmJ07tw5ffDBByV64eqaNWuUmpqqJ554wqUeT09PtW3b1qWeXEOHDnV53qlTJx05csT5fPXq1fL29tZzzz3nXObh4aHhw4cXu76BAweqUqVKLtuS5LK9gqxatUrt2rVTmzZtnMsiIiI0ePDgm24zLCxMP/zwQ5E/FitI586d1aRJkyKPz9ubkSNHSrq2H6Vp1apVatOmjTp27OhcFhQUpCFDhujo0aPav3+/y/inn37a5Rqmor4fQFnh4yrAjQ4dOqQLFy4oMjKywPUpKSkuz8ePH6/Fixdr27Ztmjp1arF+cBa1Hkl66KGHClwfEhLi8tzPzy/fRx+VKlVyuZ7o2LFjqlatmgICAlzG1a9fv9j11a5dO9+2JLlsryDHjh1T27Zt8y1v2LDhTbf58ssva+3atWrTpo3q16+v7t27a9CgQerQoUOR665bt26Rx0pSgwYNXJ7Xq1dPHh4epf59RoX1Kffuv2PHjqlZs2bO5bf6fgBlhZADuFFOTo4iIyO1cOHCAtfnDRBHjhxxBpGkpKRSqUe6dl1O1apV863Pe9bI09OzxGu4kcK2Z4wptW02btxYBw8eVEJCglavXq0lS5bonXfe0WuvvabJkycXaY6iXPtzI3kvwC7ogmxJ+S74Lm3ueD+A4iDkAG5Ur149rV27Vh06dLjpD8KcnBzFxcUpJCREY8aM0dSpU9W/f3/169fPOaawH37FqUeSIiMj1a1bt9uaK1edOnWUmJioy5cvu5zNKegunNut/0Y15IbD6x08eLBIrw8MDNTAgQM1cOBAZWZmql+/fnrjjTf0yiuvyM/Pr8TrPnTokMvZn8OHDysnJ8d5wXLuGZO8X/B37NixfHMVp7Y6deoU2JN///vfzvVARcI1OYAbDRgwQNnZ2ZoyZUq+dVlZWS4/xP785z9r06ZNmjdvnqZMmaL7779fw4YNc/kVAIGBgZLy//ArqpiYGIWEhGjq1Km6evVqvvVnz569pTmvXr2q9957z7ksJyfHebv49W63/sI88sgj2rJli7Zt2+Zcdvbs2ULPoF3vp59+cnnu4+OjJk2ayBjj7FFJ1523N7Nnz5Yk9ejRQ9K1jw3Dw8O1ceNGl3HvvPNOvrmKU9sjjzyibdu2afPmzc5lv/zyi+bNm6fo6OgS/3gUKG2cyQHcqHPnznr++ecVHx+vXbt2qXv37vL29tahQ4f0ySefaNasWerfv78OHDigP/zhD4qLi1OvXr0kXfu9Sy1atNALL7ygjz/+WNK1MzFhYWH661//quDgYAUGBqpt27ZFviYkJCREc+fO1ZNPPqn77rtPjz/+uCIiInT8+HF9+umn6tChg+bMmVOsfezTp4/atGmjsWPH6vDhw2rUqJH+9a9/6eeff5bkeqahZcuWkqRRo0YpJiZGnp6eevzxx4u1vYK89NJL+vDDD/Xwww9r9OjRzlvI69Spoz179tzwtd27d1fVqlXVoUMHRUVF6cCBA5ozZ4569uyp4OBgl7pfffVVPf744/L29lavXr2cAaO4kpOT9dhjj+nhhx/W5s2b9T//8z8aNGiQmjdv7hzz7LPPatq0aXr22WfVqlUrbdy4Ud99912+uYpT24QJE7Ro0SL16NFDo0aNUuXKlfXBBx8oOTlZS5Ys4duRUfG4+e4u4I5S2C3S8+bNMy1btjT+/v4mODjY3H333eall14yJ0+eNFlZWaZ169amZs2aJjU11eV1ubfs/uMf/3AuW7FihWnSpInx8vK66e3keW8hz5WYmGhiYmJMaGio8fPzM/Xq1TNxcXFm+/btzjGxsbEmMDAw35y5t0Ff7+zZs2bQoEEmODjYhIaGmri4OPP1118bSWbx4sXOcVlZWWbkyJEmIiLCOBwO5zy5t0xPnz493/aU55bpwuzZs8d07tzZ+Pn5mRo1apgpU6aYv/3tbze9hfzdd981DzzwgKlSpYrx9fU19erVM+PHjzcXLlxwmX/KlCmmRo0axsPDw2VOSWb48OEF1pS39tze7d+/3/Tv398EBwebSpUqmREjRpj09HSX116+fNk888wzJjQ01AQHB5sBAwaYlJSUAvtRWG15byE3xpjvv//e9O/f34SFhRk/Pz/Tpk0bk5CQ4DIm9xbyTz75xGX5jW5tB9zBYQxXiAEoe8uXL1ffvn311VdfFetOJQAoKkIOgFKXnp7ucmF1dna2unfvru3bt+v06dO3ffcRABSEa3IAlLqRI0cqPT1d7du315UrV7R06VJt2rRJU6dOJeAAKDWcyQFQ6j766CPNnDlThw8fVkZGhurXr69hw4ZpxIgR7i4NgMUIOQAAwErcDwgAAKxEyAEAAFa6oy88zsnJ0cmTJxUcHFxqXycPAABKljFGaWlpql69+g2/pPKODjknT55UrVq13F0GAAC4BSdOnFDNmjULXX9Hh5zcr2Q/ceKEQkJC3FwNAAAoiosXL6pWrVrOn+OFuaNDTu5HVCEhIYQcAAAqmJtdasKFxwAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASl7uLgA3lnzuF/1yJavQ9VeyM3Tyl2OqHlhHvp5+xZ4/0NdLdcMDb6dEAADKJUJOOZZ87hc9OGP9Dcd4+P2owLqz9UvySOVk1Lil7SSO60LQAQBYh5BTjuWewXlrYAvVjwwqcEzyxYP6/TZp1sAWqhvSsFjzH065pDH/2HXDM0UAAFRUhJwKoH5kkJrVCC1wnYfftfBTLzJITaoUPAYAgDsRFx4DAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOaUkPTNbe3+8oPTMbHeXUiHQLwBASSPklJLvz17So7O/0vdnL7m7lAqBfgEAShohBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJS93FwBIUnp6us6sekttZg9URvplSZIxRg6HQw6HQ5LkcDgUFBSk4OBgeXt7KzU1VVWqVFHDhg3VuHFj7d69WwEBAerQoYMaN26sxYsX6+jRo/Lz81Pr1q31wAMPaP/+/Tpy5IhycnIUFhYmLy8vdenSRV26dJGnp6eys7P15Zdf6tSpU6pWrZo6deokT0/Pm9afnZ2t9evXa/369ZJ0wzlbtWqlCRMm6NChQ2rQoIGmT58uf3//2+5hcWq/1f28k+V9j3N7lpKSYkUPy/qYKI/HYHmsqaIqN700xdC5c2czevTo4rykVCQmJhpJ5vz587c1z4ULF4wkc+HChZIp7DpJP6SaOi8nmKQfUkt1jn3n9plmC5qZfef2uaXGktC7d28jya2PiIgIM378eBMdHe2yPDo62ixZsuSG9S9ZssRERkYWec6CHr17976tHi5ZsqTItRdnLK4p7D22pYdlfUyUx2OwPNZUUZVFL4v687vcf1zVpUsXjRkzxt1loJT06dNHK1asKLX5K1eurNDQ0HzLq1atqkqVKkmSatSoobNnz2r69OkKDw/X5s2blZaWps2bN+vuu+9W//79tXTp0gLnX7p0qfr376+UlBR17NhR69at07p169SxY8d8c/bs2VOSnGem5syZowkTJsjHx0crVqxQnz59bmkfc2u4++67b1p7ccbimrzv8aRJkyRdO7ZyxcfHV9gelvUxUR6PwfJYU0VV7npZnOTkjjM5BW2TMzn/pyKfybl8+XKpn6Hx8PAwnp6exsfHJ9+/KjIyMkxUVJTx8vIyfn5+xtPT00RHR5usrCxnjdnZ2aZXr16mbt26LsuNMSYrK8tER0cbf39/8+ijj5rs7GznuszMTOPv7288PT1NnTp1TFpampFkfHx8THp6usucV65ccdZ3+fLlYvUwt4ZevXq5bL+g2oszFtfkfY8zMzOdPbx69ap59NFHTUBAgImOjjaZmZkVrodlfUyUx2OwPNZUUZVlL4v68/uWQ05GRoYZO3asqV69ugkICDBt2rQxiYmJzrHz5883oaGhZvXq1aZRo0YmMDDQxMTEmJMnTzrHXL161YwcOdKEhoaaypUrm5deesk89dRTzlP3sbGx+X5oJScnO0PO2rVrTcuWLY2/v79p3769+fe//33D+jMyMsyFCxecjxMnTpRayPkm+SdT5+UEs+zbH0zSD6m39Fj27Q+mzssJ5pvknwrdzu2EnJKo8XYej8c+WyJBJjw8/KZjnnjiiXzLEhMTzbvvvlvg8utt2rSpwOW5x6Eks3nz5kLXSTJ9+vQxksyECRMKnPOll14ykszw4cOL9R7mbifv9guqvThjcU3e9zhvD3N7ltu3itbDsj4myuMxWB5rqqjKspdFDTm3fOHxiBEjtH//fi1evFjVq1fXsmXL9PDDDyspKUkNGjSQJF2+fFkzZszQhx9+KA8PD/32t7/VuHHjtHDhQknSn/70Jy1cuFDz589X48aNNWvWLC1fvlwPPvigJGnWrFn67rvv1KxZM73++uuSpIiICB09elSS9Oqrr2rmzJmKiIjQ0KFD9bvf/U5ff/11oTXHx8dr8uTJt7rLxfLD+XRJ0ph/7CqRuVpF3/Y0Bc4rlUyNt+LMhh0lMk9wcLDOnTt3wzGtWrXSokWLXJadOnVKjz76aL6xp06dcnnerFmzApdf/zx3TGFzfP/995KkZ599tsA5n3nmGb355ps6dOjQDfejsFrzbv9GtRdn7J0ub99Wrlzp/PP1/80dm3s8VZQe3srxU5G2V1FrqqjKYy9vKeQcP35c8+fP1/Hjx1W9enVJ0rhx47R69WrNnz9fU6dOlSRdvXpVf/3rX1WvXj1J14JRbliRpNmzZ+uVV15R3759JV27RmHVqlXO9aGhofLx8VFAQICqVq2ar4433nhDnTt3liRNmDBBPXv2VEZGhvz8/Aqs+5VXXtGLL77ofH7x4kXVqlXrVlpwUzUrXbtb5q2BLVQ/MuiW5jiccklj/rHLOVdJK4kab8cbp1tq8dGdtz1PWlraTcds374937Jq1aopISGhwOXX27t3b4HLr3++d+9etWvXrtA56tWrp6SkJL3//vuKj4/PN+ff/vY3SXL+A6Gocl+fd/s3qr04Y+90efuWt9+5PcsdW9F6eCvHT0XaXkWtqaIql70szumh3I+rEhISjCQTGBjo8vDy8jIDBgwwxlz7uCogIMDl9UuXLjUOh8MYY0xqaqqRZDZs2OAypm/fvi53mtzompyUlBTnsm+//dZIMseOHSvy/nBNDtfkcE3OzffzTsY1OVyTU9HeU3cqj9fk3NLdVZcuXZKnp6d27NihXbt2OR8HDhzQrFmznOO8vb1dXudwOGSMuZVNFuj6+XPvWMnJySmx+VG6/P391bt371LdRlhYmIKCgpSZmemyPCMjQ9WqVdOZM2cUFRWljIwMZWdnKzw8XNu2bXPeEdCnTx8lJCRoxowZ+b7jwdPTUzNnzlRGRoYSEhL0wAMPaO3atVq7dq0eeughpaenKzs7WxEREUpKSlLPnj2VmZmpgIAArVy5Ui+++KJ+//vfKzg4WJmZmerdu3exvy8nt4aEhAT16dPH5W6GvLUXZyyuyfseP/jgg4qNjdXKlSsVGRmphIQEXb58WUOGDNFvfvObCtfDsj4myuMxWB5rqqjKZS+Lk5xyz6ocPHjQSDIbN24sdGzuhcfXW7Zsmbl+k1FRUWbGjBnO51lZWaZOnTouZ3J+/etfmxEjRrjMU9DdVTt37nRemFxUnMnhe3JyH5GRkQV+p03dunVv+XtyCpuzoEdpfE9OYbUXZyyuKcr35FTkHpb1MVEej8HyWFNFVRa9LNULj++66y4NHjxYTz31lGbOnKl7771XZ8+e1bp163TPPfc4vw/kZkaOHKn4+HjVr19fjRo10uzZs3X+/HnnWRlJio6O1tatW3X06FEFBQW5fDcF7LB8+XJ9c/i0Huj3pBzfb3brNx7Hx8cX+1s6+/Xrp969exf6jcd55yyNbzzOraEotRdnLK4p6D226RuPy/qYKI/HYHmsqaIqT7285bur5s+frz/+8Y8aO3asfvzxR4WHh6tdu3YF3q1SmJdfflmnT5/WU089JU9PTw0ZMkQxMTEujRg3bpxiY2PVpEkTpaenKzk5+VZLRjnm7++vqEfGKGHkP9WsRmiJzFnQsdijR48bvsbT01NdunQp9rY8PT3VtWtXde3atUhzzpkzp9jbKEoNRa39VvfzTnaj99gGZX1MlMdjsDzWVFGVl14WK+Tk/gtGunY9zOTJkwu9JTsuLk5xcXEuy/r06eNyTY6Xl5dmz56t2bNnS7p2PU3jxo01YMAA55i77rpLmzdvdpknOjo637U9LVq0KNHrfQAAQMXm1l/QeezYMX3++efq3Lmzrly5ojlz5ig5OVmDBg1yZ1kAAMACbv3dVR4eHlqwYIFat26tDh06KCkpSWvXrlXjxo3dWRYAALCAW8/k1KpV64bfUAwAAHCryv1vIQcAALgVhBwAAGAlQg4AALASIQcAAFiJkFNK6kUEKWFkR9WLKPvf7l0R0S8AQElz691VNvP38Syxb+69E9AvAEBJ40wOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJX+tQjqVfzZYk7f3xQqFjki9ekiR9n3JJORmFjyvI4ZRLt14cAADlHCGnHPv+/4eQCUuTCh3j4fejAutKo/+xSzkZZ29pO4G+HAYAAPvw060c6960qiSpXmSQ/L09CxxzJTtDJ39poeoP1pGvp1+xtxHo66W64YG3VScAAOURIaccqxzoo8fb1L7JqFC1VFSZ1AMAQEXChccAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEpe7i7AnYwxkqSLFy+6uRIAAFBUuT+3c3+OF+aODjlpaWmSpFq1arm5EgAAUFxpaWkKDQ0tdL3D3CwGWSwnJ0cnT55UcHCwHA5HkV5z8eJF1apVSydOnFBISEgpV3hno9dlh16XHXpdduh12SnrXhtjlJaWpurVq8vDo/Arb+7oMzkeHh6qWbPmLb02JCSEvzRlhF6XHXpdduh12aHXZacse32jMzi5uPAYAABYiZADAACsRMgpJl9fX02cOFG+vr7uLsV69Lrs0OuyQ6/LDr0uO+W113f0hccAAMBenMkBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQk4xvf3224qOjpafn5/atm2rbdu2ubukcm3jxo3q1auXqlevLofDoeXLl7usN8botddeU7Vq1eTv769u3brp0KFDLmN+/vlnDR48WCEhIQoLC9MzzzyjS5cuuYzZs2ePOnXqJD8/P9WqVUtvvvlmae9auRMfH6/WrVsrODhYkZGR6tOnjw4ePOgyJiMjQ8OHD1eVKlUUFBSk3/zmNzpz5ozLmOPHj6tnz54KCAhQZGSkxo8fr6ysLJcx69ev13333SdfX1/Vr19fCxYsKO3dKzfmzp2re+65x/nNru3bt9dnn33mXE+PS8+0adPkcDg0ZswY5zL6XTImTZokh8Ph8mjUqJFzfYXts0GRLV682Pj4+Jj//u//Nvv27TPPPfecCQsLM2fOnHF3aeXWqlWrzKuvvmqWLl1qJJlly5a5rJ82bZoJDQ01y5cvN7t37zaPPfaYqVu3rklPT3eOefjhh03z5s3Nli1bzJdffmnq169vnnjiCef6CxcumKioKDN48GCzd+9es2jRIuPv72/efffdstrNciEmJsbMnz/f7N271+zatcs88sgjpnbt2ubSpUvOMUOHDjW1atUy69atM9u3bzft2rUz999/v3N9VlaWadasmenWrZvZuXOnWbVqlQkPDzevvPKKc8yRI0dMQECAefHFF83+/fvN7Nmzjaenp1m9enWZ7q+7/Otf/zKffvqp+e6778zBgwfN73//e+Pt7W327t1rjKHHpWXbtm0mOjra3HPPPWb06NHO5fS7ZEycONE0bdrUnDp1yvk4e/asc31F7TMhpxjatGljhg8f7nyenZ1tqlevbuLj491YVcWRN+Tk5OSYqlWrmunTpzuXpaamGl9fX7No0SJjjDH79+83ksw333zjHPPZZ58Zh8NhfvzxR2OMMe+8846pVKmSuXLlinPMyy+/bBo2bFjKe1S+paSkGElmw4YNxphrvfX29jaffPKJc8yBAweMJLN582ZjzLVQ6uHhYU6fPu0cM3fuXBMSEuLs70svvWSaNm3qsq2BAweamJiY0t6lcqtSpUrm/fffp8elJC0tzTRo0MCsWbPGdO7c2Rly6HfJmThxomnevHmB6ypyn/m4qogyMzO1Y8cOdevWzbnMw8ND3bp10+bNm91YWcWVnJys06dPu/Q0NDRUbdu2dfZ08+bNCgsLU6tWrZxjunXrJg8PD23dutU55oEHHpCPj49zTExMjA4ePKjz58+X0d6UPxcuXJAkVa5cWZK0Y8cOXb161aXfjRo1Uu3atV36fffddysqKso5JiYmRhcvXtS+ffucY66fI3fMnfj3IDs7W4sXL9Yvv/yi9u3b0+NSMnz4cPXs2TNfT+h3yTp06JCqV6+uX/3qVxo8eLCOHz8uqWL3mZBTROfOnVN2drbLGyhJUVFROn36tJuqqthy+3ajnp4+fVqRkZEu6728vFS5cmWXMQXNcf027jQ5OTkaM2aMOnTooGbNmkm61gsfHx+FhYW5jM3b75v1srAxFy9eVHp6emnsTrmTlJSkoKAg+fr6aujQoVq2bJmaNGlCj0vB4sWL9e233yo+Pj7fOvpdctq2basFCxZo9erVmjt3rpKTk9WpUyelpaVV6D57lcqsANxq+PDh2rt3r7766it3l2Klhg0bateuXbpw4YL++c9/KjY2Vhs2bHB3WdY5ceKERo8erTVr1sjPz8/d5VitR48ezj/fc889atu2rerUqaOPP/5Y/v7+bqzs9nAmp4jCw8Pl6emZ72ryM2fOqGrVqm6qqmLL7duNelq1alWlpKS4rM/KytLPP//sMqagOa7fxp1kxIgRSkhIUGJiomrWrOlcXrVqVWVmZio1NdVlfN5+36yXhY0JCQmp0P8zLA4fHx/Vr19fLVu2VHx8vJo3b65Zs2bR4xK2Y8cOpaSk6L777pOXl5e8vLy0YcMG/eUvf5GXl5eioqLodykJCwvTXXfdpcOHD1fo45qQU0Q+Pj5q2bKl1q1b51yWk5OjdevWqX379m6srOKqW7euqlat6tLTixcvauvWrc6etm/fXqmpqdqxY4dzzBdffKGcnBy1bdvWOWbjxo26evWqc8yaNWvUsGFDVapUqYz2xv2MMRoxYoSWLVumL774QnXr1nVZ37JlS3l7e7v0++DBgzp+/LhLv5OSklyC5Zo1axQSEqImTZo4x1w/R+6YO/nvQU5Ojq5cuUKPS1jXrl2VlJSkXbt2OR+tWrXS4MGDnX+m36Xj0qVL+v7771WtWrWKfVyX2iXNFlq8eLHx9fU1CxYsMPv37zdDhgwxYWFhLleTw1VaWprZuXOn2blzp5Fk/vznP5udO3eaY8eOGWOu3UIeFhZmVqxYYfbs2WN69+5d4C3k9957r9m6dav56quvTIMGDVxuIU9NTTVRUVHmySefNHv37jWLFy82AQEBd9wt5MOGDTOhoaFm/fr1LreBXr582Tlm6NChpnbt2uaLL74w27dvN+3btzft27d3rs+9DbR79+5m165dZvXq1SYiIqLA20DHjx9vDhw4YN5+++076nbbCRMmmA0bNpjk5GSzZ88eM2HCBONwOMznn39ujKHHpe36u6uMod8lZezYsWb9+vUmOTnZfP3116Zbt24mPDzcpKSkGGMqbp8JOcU0e/ZsU7t2bePj42PatGljtmzZ4u6SyrXExEQjKd8jNjbWGHPtNvI//OEPJioqyvj6+pquXbuagwcPuszx008/mSeeeMIEBQWZkJAQ8/TTT5u0tDSXMbt37zYdO3Y0vr6+pkaNGmbatGlltYvlRkF9lmTmz5/vHJOenm5eeOEFU6lSJRMQEGD69u1rTp065TLP0aNHTY8ePYy/v78JDw83Y8eONVevXnUZk5iYaFq0aGF8fHzMr371K5dt2O53v/udqVOnjvHx8TERERGma9euzoBjDD0ubXlDDv0uGQMHDjTVqlUzPj4+pkaNGmbgwIHm8OHDzvUVtc8OY4wpvfNEAAAA7sE1OQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACw0v8D7klIw81asqoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df[\"length\"].plot(kind=\"box\", vert = False)\n",
    "plt.title(\"Text length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of outliers with very long reports. We see that 555 could be a more reasonable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxs8QE1Ha7ld",
    "tags": []
   },
   "source": [
    "### 2  Classifying with a Recurrent Neural Network\n",
    "\n",
    "We import some pytorch submodules (PROVIDED)\n",
    "\n",
    "* torch: functions to create tensors and operations on tensors \n",
    "* torch.nn: to specify neural networks\n",
    "* torch.nn.functional for when we want to define a custom layer for example with a convolution operation layer.\n",
    "* torch.optim: optimizers for training \n",
    "\n",
    "We also need to specify some constants which ensure that the code can run on CPU. The `max_len` constant is important as text whose size exceeds (in number of tokens) that limit will be discarded. We set it to 555 (see next). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.1/1.2 MB 3.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.2 MB 2.4 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.3/1.2 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.5/1.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.9/1.2 MB 3.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.1/1.2 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.2/1.2 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torchaudio in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0) (3.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0) (3.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "  Downloading torchvision-0.15.1-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.2 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.4/1.2 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.6/1.2 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.9/1.2 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.2/1.2 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.0.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bleuze3u\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.15.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch==2.0.0 torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SU-ibO_-a7ll"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C9UoOQka7l7"
   },
   "source": [
    "#### Exercise 4 - Creating tensors\n",
    "\n",
    "To help speed up computation, all data must be converted to tensors. \n",
    "\n",
    "* Set the maximum size of text to the 3rd quartile (555)\n",
    "* Create a tensor of size  (number of texts, maximum size of a text) for X. Call this tensor X. Longer texts will be cut down to maximal size and shorter texts will be padded with `<eos>`. Use torch.zeros method and make sure to specify the components will be of type integer (long attribute in torch)\n",
    "* Populate this matrix with the integer version of the BBC news report (cf. Exercise 1). Use the torch.LongTensor method. When populating the matrix cut down sentence whose length is above the max length to max length\n",
    "* Create another tensor of size (numbers of text) and populate it with the list of labels. Call this tensor Y.\n",
    "* Print out the shape of X and Y . X should be of size (2225, 555) and Y of size (2225,) \n",
    "\n",
    "N.B. In practice, it is not necessary to have all the data in a single tensor. In fact, this is inefficient if the texts have varied length. The only constraint is that for a given batch, all sequences have the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    }
   ],
   "source": [
    "max_length = 555 #3rd quartile of the length distribution\n",
    "num_texts = len(texts_as_ints)\n",
    "print(num_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2225, 555])\n"
     ]
    }
   ],
   "source": [
    "# create tensor X of integers with shape (num_texts, max_length)\n",
    "X = torch.zeros(num_texts, max_length).long()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2225, 555])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[27237, 28574, 14131,  ...,     0,     0,     0],\n",
       "        [27597, 17213, 10454,  ...,     0,     0,     0],\n",
       "        [27016, 26658,  1644,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [20215, 12846,  5490,  ...,     0,     0,     0],\n",
       "        [ 5669, 18693, 25759,  ..., 21964, 25214, 27757],\n",
       "        [ 2723, 10978, 10042,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# populate X with the data\n",
    "for i in range(num_texts):\n",
    "    text_as_ints = texts_as_ints[i]\n",
    "    length = len(text_as_ints)\n",
    "    if length <= max_length:\n",
    "        X[i, :length] = torch.tensor(text_as_ints)\n",
    "    else:\n",
    "        X[i, :] = torch.tensor(text_as_ints[:max_length])\n",
    "\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2225])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.LongTensor(labels)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eakhMa5Sa7mL"
   },
   "source": [
    "#### Exercise 5 - Create train and test data\n",
    "\n",
    "* Split X into two parts, one called X_train which consists of the first 1112 items and the other called X_valid which includes the rest of the data\n",
    "* Do the same for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1112, 555]) torch.Size([222, 555])\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:1112]\n",
    "X_valid = X[1112:1334]\n",
    "\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1112]) torch.Size([222])\n"
     ]
    }
   ],
   "source": [
    "Y_train = Y[:1112]\n",
    "Y_valid = Y[1112:1334]\n",
    "\n",
    "print(Y_train.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZkXMuF0a7mj"
   },
   "source": [
    "#### Create batches with DataLoader (PROVIDED)\n",
    "\n",
    "pytorch provides a batch generator which shuffles the data. We apply it to train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veYFlstja7mm"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 32\n",
    "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
    "# Note that input_features and labels must match on the length of the first dimension\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwh1VKE1a7n1"
   },
   "source": [
    "#### Exercise 6 - Define your neural network (PARTIALLY PROVIDED)\n",
    "\n",
    "We define a network by specifying a subclass of the appropriate pytorch modules. For instance here, as we want to create an RNN we create a subclass of pytorch RNN module. The specification of the network falls into 2 parts. \n",
    "\n",
    "In the **init part**, the layers of the network are defined and  their type and shape are specified.  \n",
    "\n",
    "In the **forward part**, we connect the layers and specify input and output for each layer. \n",
    " \n",
    "* The hidden state returned by the GRU layer is of size (num_layers * num_directions, batch_size, hidden_size) \n",
    "* The input to the decision layer should be of size (batch_size, hidden_size).   \n",
    "\n",
    "Hence the first 2 dimensions of hidden must be transposed and the tensor redimensioned to (batch_size, hidden_size).\n",
    "\n",
    "* drop : (num_layers * num_directions, batch_size, hidden_size)\n",
    "* drop.transpose(0,1): (batch_size, num_layers * num_directions,  hidden_size)\n",
    "* x.size(0) = batch_size\n",
    "* drop.transpose(0,1).contiguous.view(x.size(0), -1): (batchsize, hiddensize)\n",
    "\n",
    "**TODO:** fill in the missing variables (marked by ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyV6gWoOa7n7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(34731, 64)\n",
       "  (rnn): GRU(64, 128, batch_first=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (decision): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Here we define the network layers\n",
    "        \n",
    "        # An embedding layer projecting vectors of size vocab_size into embeddings of size embed_size\n",
    "        # Assigns to each word in the vocabulary an embedding of size embed_size\n",
    "        self.embed = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embed_size)\n",
    "        \n",
    "        # A recurrent (GRU) layer to process each input token (represented by its embedding)\n",
    "        # The GRU network takes as input the embedding (of size embed_size) of the current word \n",
    "        # and the previous hidden state (of size hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True) \n",
    "        \n",
    "        # Drop out layer for regularisation\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Fully connected layer mapping \n",
    "        # the last layer maps a hidden state to a vector of size the number of classes\n",
    "        self.decision = nn.Linear(in_features = hidden_size, out_features = num_classes)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # Here we say how the layers are connected \n",
    "       \n",
    "        #  for each token in the input, retrieve the corresponding embeddings \n",
    "        \n",
    "        # x = [batch size, input size]\n",
    "        embed = self.embed(x)\n",
    "        # embed = [batch size, sent len, emb dim]       \n",
    "       \n",
    "        # Run the RNN on the input embeddings\n",
    "        # output is the sequence of hidden states produced by the RNN\n",
    "        # hidden is the last hidden state produced\n",
    "        output, hidden = self.rnn(embed)\n",
    "        \n",
    "        # output = [sent len, batch size, hidden size]\n",
    "        # hidden = [num_layers * num_directions, batch size, hidden_size ]\n",
    "        \n",
    "        # Apply dropout (for regularisation)\n",
    "        drop = self.dropout(hidden)\n",
    "        \n",
    "        # drop = [num_layers * num_directions, batch size, hidden_size]   \n",
    "        \n",
    "        # Apply the fully connected layer to the output of the dropout\n",
    "        # Expected input size: [batch_size, input_size]\n",
    "        # We transpose [num_layers * num_directions, batch size, hidden_size ]\n",
    "        # to: [batch size, num_layers * num_directions, hidden_size ]\n",
    "        # And apply view to create an input of the form [batch size, input_size ]\n",
    "        # (x.size(0) = batch size)\n",
    "        \n",
    "        # effectively the same as with drop.squeeze(0)\n",
    "        return self.decision(drop.view(x.size(0), -1))\n",
    "    \n",
    "rnn_model = RNN(vocab_size = len(tokens2int), embed_size = 64, \n",
    "                hidden_size = 128, num_classes=len(df.labels.unique()))\n",
    "device = torch.device('cpu')\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3276ujaa7nF"
   },
   "source": [
    "#### Exercise 7  - Evaluating (PARTIALLY PROVIDED)\n",
    "\n",
    "We define an evaluation function (called \"perf\") which computes the average loss on the test/validation dataset and the proportion of correct cases. \n",
    "\n",
    "* We use pytorch nn.CrossEntropyLoss() as loss function.\n",
    "* For each batch returned by the loader, we calculate the scores produced by the model for each class, the loss, the predictions and the loss.\n",
    "* To block dropout (which should only be used at training time), we use the eval() method. \n",
    "* \"with torch.no_grad()\" temporarily set all the requires_grad flag to false. In practice, this enforces that gradients are not computed (and therefore the weights of the model remain unchanged). \n",
    "\n",
    "**TODO:** Modify the function so that it outputs the proportion of correct predictions in addition to the loss. Replace the '??' in the function with the correct variables and/or operations on these variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WW8p4Pnga7nH"
   },
   "outputs": [],
   "source": [
    "def perf(model, loader):\n",
    "    # define the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # No drop out\n",
    "    model.eval()\n",
    "    total_loss, correct, num = 0, 0, 0\n",
    "    with torch.no_grad():  # No gradient computation, weights remain unchanged\n",
    "        for x, y in loader:\n",
    "\n",
    "            # Compute the scores for the instances in the input batch\n",
    "            y_scores = model(x)\n",
    "            # Compute the loss\n",
    "            loss = criterion(y_scores, y)\n",
    "            # Compute the predictions\n",
    "            y_pred = torch.max(y_scores, 1)[1] # same as .argmax(dim=1)\n",
    "\n",
    "            # Update the batch loss\n",
    "            total_loss += loss.item()\n",
    "            num += len(y)\n",
    "            \n",
    "            # check for match with target \n",
    "            # sum Trues to give count of correct predictions\n",
    "            correct += sum(y_pred == y).item()\n",
    "    \n",
    "    return {'loss': total_loss, 'acc': correct/num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 11.366937398910522, 'acc': 0.15765765765765766}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 57.23272728919983, 'acc': 0.1447841726618705}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf(rnn_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8  - Training Loop\n",
    "\n",
    "Define a function fit(model, epochs) which you will use to train your model\n",
    "\n",
    "* use the CrossEntropyLoss and the Adam optimizer\n",
    "* iterates over the epochs and for each epoch:\n",
    "   - Set the module in training mode (use the train() method). Dropout will be enabled and gradients will be computed. \n",
    "   - Initialise the total loss to 0\n",
    "   - Iterate over each batches returned by train_loader    \n",
    "     For each batch:   \n",
    "        - set the gradients to null (optimizer.zero_grad())\n",
    "        - predicts the batch scores\n",
    "        - calculate the loss\n",
    "        - back propagate\n",
    "        - optimize (adjust the weights)\n",
    "        - update the total loss\n",
    "   \n",
    "**Hint:** Some of these steps are defined in the preceding exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epochs):\n",
    "\n",
    "    # define the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define the optimiser\n",
    "    optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # iterate over epochs    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # set the model in training mode\n",
    "        model.train()\n",
    "\n",
    "        # initialize the total_loss to 0\n",
    "        total_loss = 0\n",
    "\n",
    "        # iterate over batches\n",
    "        for x, y in train_loader:\n",
    "                \n",
    "            # reset the gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # predict the batch scores\n",
    "            y_scores = model(x)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(y_scores, y)\n",
    "\n",
    "            # Compute the gradients (backpropagation)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights (optimization)\n",
    "            optimiser.step()\n",
    "\n",
    "            # Update the batch loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"Epoch: \", epoch, \", Total loss: \", total_loss)\n",
    "        print(\" - Train: \", perf(model, train_loader))\n",
    "        print(\" - Validation: \", perf(model, valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nlDoZhxa7oo"
   },
   "source": [
    "#### Exercise 9 - Training\n",
    "\n",
    "Use the fit function you just defined to train your model. \n",
    "* The loss function on the train and validation set should decrease\n",
    "* The accuracy on the validation set should improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , Total loss:  53.12880790233612\n",
      " - Train:  {'loss': 52.14663338661194, 'acc': 0.33902877697841727}\n",
      " - Validation:  {'loss': 10.932819604873657, 'acc': 0.2882882882882883}\n",
      "Epoch:  1 , Total loss:  51.656761169433594\n",
      " - Train:  {'loss': 49.99267637729645, 'acc': 0.3489208633093525}\n",
      " - Validation:  {'loss': 10.96294367313385, 'acc': 0.28378378378378377}\n",
      "Epoch:  2 , Total loss:  50.06919348239899\n",
      " - Train:  {'loss': 50.053454756736755, 'acc': 0.3902877697841727}\n",
      " - Validation:  {'loss': 11.37528145313263, 'acc': 0.26126126126126126}\n",
      "Epoch:  3 , Total loss:  49.144044399261475\n",
      " - Train:  {'loss': 46.89612567424774, 'acc': 0.38758992805755393}\n",
      " - Validation:  {'loss': 11.164799571037292, 'acc': 0.2972972972972973}\n",
      "Epoch:  4 , Total loss:  46.97850060462952\n",
      " - Train:  {'loss': 45.50417184829712, 'acc': 0.42356115107913667}\n",
      " - Validation:  {'loss': 11.175099611282349, 'acc': 0.2747747747747748}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "fit(rnn_model, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 11.175099611282349, 'acc': 0.2747747747747748}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model on the validation set\n",
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Classifying with a Convolutional Neural Network (OPTIONAL)\n",
    "\n",
    "Here below is a CNN model definition. Run it on the data and compare the speed and results with that of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embed): Embedding(34731, 64)\n",
       "  (conv): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (decision): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.conv = nn.Conv1d(embed_size, hidden_size, kernel_size=2)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.decision = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        conv = F.relu(self.conv(embed.transpose(1,2)))\n",
    "        pool = F.max_pool1d(conv, conv.size(2))\n",
    "        drop = self.dropout(pool)\n",
    "        return self.decision(drop.view(x.size(0), -1))\n",
    "\n",
    "cnn_model = CNN(vocab_size = len(tokens2int), embed_size = 64, \n",
    "                hidden_size = 128, num_classes=len(df.labels.unique()))\n",
    "cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got multiple values for argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fit(cnn_model, train_loader, epochs \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got multiple values for argument 'epochs'"
     ]
    }
   ],
   "source": [
    "fit(cnn_model, train_loader, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_text-classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
